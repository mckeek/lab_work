{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, re, argparse, urllib.request, html5lib\n",
    "from bs4 import BeautifulSoup, Tag, UnicodeDammit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_url = 'http://www.imsdb.com/scripts/Aliens.html'\n",
    "\n",
    "request = urllib.request.Request(script_url)\n",
    "\n",
    "webpage_bytes = urllib.request.urlopen(request)\n",
    "soup = BeautifulSoup(webpage_bytes, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"Each time we gather to inaugurate a president, we bear witness to the enduring strength of our Constitution. We affirm the promise of our democracy. We recall that what binds this nation together is not the colors of our skin or the tenets of our faith or the origins of our names. What makes us exceptional – what makes us American – is our allegiance to an idea, articulated in a declaration made more than two centuries ago:    “We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable rights, that among these are Life, Liberty, and the pursuit of Happiness. For we, the people, understand that our country cannot succeed when a shrinking few do very well and a growing many barely make it. We believe that America’s prosperity must rest upon the broad shoulders of a rising middle class. We know that America thrives when every person can find independence and pride in their work; when the wages of honest labor liberate families from the brink of hardship. We, the people, still believe that every citizen deserves a basic measure of security and dignity. We must make the hard choices to reduce the cost of health care and the size of our deficit. But we reject the belief that America must choose between caring for the generation that built this country and investing in the generation that will build its future. We, the people, still believe that our obligations as Americans are not just to ourselves, but to all posterity. We will respond to the threat of climate change, knowing that the failure to do so would betray our children and future generations. We will defend our people and uphold our values through strength of arms and rule of law. We will show the courage to try and resolve our differences with other nations peacefully – not because we are naïve about the dangers we face, but because engagement can more durably lift suspicion and fear. America will remain the anchor of strong alliances in every corner of the globe; and we will renew those institutions that extend our capacity to manage crisis abroad, for no one has a greater stake in a peaceful world than its most powerful nation. Our journey is not complete until our wives, our mothers, and daughters can earn a living equal to their efforts. Our journey is not complete until our gay brothers and sisters are treated like anyone else under the law – for if we are truly created equal, then surely the love we commit to one another must be equal as well. Our journey is not complete until no citizen is forced to wait for hours to exercise the right to vote.  Our journey is not complete until we find a better way to welcome the striving, hopeful immigrants who still see America as a land of opportunity; until bright young students and engineers are enlisted in our workforce rather than expelled from our country.\"\n",
    "corpus = corpus.lower()\n",
    "book = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaces_regex = re.compile(\"^(\\s*).*\")\n",
    "location_regex = re.compile(\"^\\s*(INT\\.|EXT\\.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: [our] Frequency: 23\n",
      "Word: [people] Frequency: 4\n",
      "Word: [journey] Frequency: 4\n"
     ]
    }
   ],
   "source": [
    "def tokenize():\n",
    "    if book is not None:\n",
    "        words = book.lower().split()\n",
    "        return words\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "\n",
    "def map_book(tokens):\n",
    "    hash_map = {}\n",
    "\n",
    "    if tokens is not None:\n",
    "        for element in tokens:\n",
    "            # Remove Punctuation\n",
    "            word = element.replace(\",\",\"\")\n",
    "            word = word.replace(\".\",\"\")\n",
    "\n",
    "            # Word Exist?\n",
    "            if word in hash_map:\n",
    "                hash_map[word] = hash_map[word] + 1\n",
    "            else:\n",
    "                hash_map[word] = 1\n",
    "\n",
    "        return hash_map\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Tokenize the Book\n",
    "words = tokenize()\n",
    "word_list = ['our','people','journey']\n",
    "\n",
    "# Create a Hash Map (Dictionary)\n",
    "map = map_book(words)\n",
    "\n",
    "# Show Word Information\n",
    "for word in word_list:\n",
    "    print('Word: [' + word + '] Frequency: ' + str(map[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'each': {'time': 1.0}, 'time': {'we': 1.0}, 'we': {'reject': 0.058823529411764705, 'face,': 0.058823529411764705, 'believe': 0.058823529411764705, 'bear': 0.058823529411764705, 'commit': 0.058823529411764705, 'will': 0.23529411764705882, 'recall': 0.058823529411764705, 'are': 0.11764705882352941, 'know': 0.058823529411764705, 'find': 0.058823529411764705, 'gather': 0.058823529411764705, 'must': 0.058823529411764705, 'affirm': 0.058823529411764705}, 'gather': {'to': 1.0}, 'to': {'the': 0.11764705882352941, 'manage': 0.058823529411764705, 'reduce': 0.058823529411764705, 'one': 0.058823529411764705, 'welcome': 0.058823529411764705, 'wait': 0.058823529411764705, 'exercise': 0.058823529411764705, 'their': 0.058823529411764705, 'an': 0.058823529411764705, 'try': 0.058823529411764705, 'be': 0.058823529411764705, 'inaugurate': 0.058823529411764705, 'do': 0.058823529411764705, 'vote.': 0.058823529411764705, 'ourselves,': 0.058823529411764705, 'all': 0.058823529411764705}, 'inaugurate': {'a': 1.0}, 'a': {'living': 0.09090909090909091, 'growing': 0.09090909090909091, 'basic': 0.09090909090909091, 'better': 0.09090909090909091, 'land': 0.09090909090909091, 'shrinking': 0.09090909090909091, 'greater': 0.09090909090909091, 'declaration': 0.09090909090909091, 'peaceful': 0.09090909090909091, 'president,': 0.09090909090909091, 'rising': 0.09090909090909091}, 'president,': {'we': 1.0}, 'bear': {'witness': 1.0}, 'witness': {'to': 1.0}, 'the': {'law': 0.03571428571428571, 'right': 0.03571428571428571, 'tenets': 0.03571428571428571, 'belief': 0.03571428571428571, 'threat': 0.03571428571428571, 'globe;': 0.03571428571428571, 'colors': 0.03571428571428571, 'brink': 0.03571428571428571, 'generation': 0.07142857142857142, 'dangers': 0.03571428571428571, 'cost': 0.03571428571428571, 'enduring': 0.03571428571428571, 'love': 0.03571428571428571, 'broad': 0.03571428571428571, 'pursuit': 0.03571428571428571, 'striving,': 0.03571428571428571, 'people,': 0.10714285714285714, 'origins': 0.03571428571428571, 'anchor': 0.03571428571428571, 'promise': 0.03571428571428571, 'failure': 0.03571428571428571, 'courage': 0.03571428571428571, 'hard': 0.03571428571428571, 'size': 0.03571428571428571, 'wages': 0.03571428571428571}, 'enduring': {'strength': 1.0}, 'strength': {'of': 1.0}, 'of': {'the': 0.05555555555555555, 'climate': 0.05555555555555555, 'security': 0.05555555555555555, 'a': 0.05555555555555555, 'our': 0.3333333333333333, 'health': 0.05555555555555555, 'arms': 0.05555555555555555, 'law.': 0.05555555555555555, 'honest': 0.05555555555555555, 'opportunity;': 0.05555555555555555, 'strong': 0.05555555555555555, 'happiness.': 0.05555555555555555, 'hardship.': 0.05555555555555555}, 'our': {'deficit.': 0.043478260869565216, 'journey': 0.17391304347826086, 'obligations': 0.043478260869565216, 'names.': 0.043478260869565216, 'values': 0.043478260869565216, 'capacity': 0.043478260869565216, 'mothers,': 0.043478260869565216, 'allegiance': 0.043478260869565216, 'wives,': 0.043478260869565216, 'children': 0.043478260869565216, 'gay': 0.043478260869565216, 'constitution.': 0.043478260869565216, 'workforce': 0.043478260869565216, 'country.': 0.043478260869565216, 'people': 0.043478260869565216, 'democracy.': 0.043478260869565216, 'country': 0.043478260869565216, 'differences': 0.043478260869565216, 'faith': 0.043478260869565216, 'skin': 0.043478260869565216}, 'constitution.': {'we': 1.0}, 'affirm': {'the': 1.0}, 'promise': {'of': 1.0}, 'democracy.': {'we': 1.0}, 'recall': {'that': 1.0}, 'that': {'the': 0.07142857142857142, 'extend': 0.07142857142857142, 'among': 0.07142857142857142, 'america’s': 0.07142857142857142, 'every': 0.07142857142857142, 'what': 0.07142857142857142, 'they': 0.07142857142857142, 'built': 0.07142857142857142, 'our': 0.14285714285714285, 'will': 0.07142857142857142, 'america': 0.14285714285714285, 'all': 0.07142857142857142}, 'what': {'binds': 0.3333333333333333, 'makes': 0.6666666666666666}, 'binds': {'this': 1.0}, 'this': {'country': 0.5, 'nation': 0.5}, 'nation': {'together': 1.0}, 'together': {'is': 1.0}, 'is': {'not': 0.7142857142857143, 'our': 0.14285714285714285, 'forced': 0.14285714285714285}, 'not': {'the': 0.14285714285714285, 'just': 0.14285714285714285, 'complete': 0.5714285714285714, 'because': 0.14285714285714285}, 'colors': {'of': 1.0}, 'skin': {'or': 1.0}, 'or': {'the': 1.0}, 'tenets': {'of': 1.0}, 'faith': {'or': 1.0}, 'origins': {'of': 1.0}, 'names.': {'what': 1.0}, 'makes': {'us': 1.0}, 'us': {'american': 0.5, 'exceptional': 0.5}, 'exceptional': {'–': 1.0}, '–': {'not': 0.25, 'for': 0.25, 'is': 0.25, 'what': 0.25}, 'american': {'–': 1.0}, 'allegiance': {'to': 1.0}, 'an': {'idea,': 1.0}, 'idea,': {'articulated': 1.0}, 'articulated': {'in': 1.0}, 'in': {'the': 0.16666666666666666, 'every': 0.16666666666666666, 'a': 0.3333333333333333, 'our': 0.16666666666666666, 'their': 0.16666666666666666}, 'declaration': {'made': 1.0}, 'made': {'more': 1.0}, 'more': {'than': 0.5, 'durably': 0.5}, 'than': {'its': 0.3333333333333333, 'two': 0.3333333333333333, 'expelled': 0.3333333333333333}, 'two': {'centuries': 1.0}, 'centuries': {'ago:': 1.0}, 'ago:': {'“we': 1.0}, '“we': {'hold': 1.0}, 'hold': {'these': 1.0}, 'these': {'truths': 0.5, 'are': 0.5}, 'truths': {'to': 1.0}, 'be': {'self-evident,': 0.5, 'equal': 0.5}, 'self-evident,': {'that': 1.0}, 'all': {'posterity.': 0.5, 'men': 0.5}, 'men': {'are': 1.0}, 'are': {'endowed': 0.125, 'naïve': 0.125, 'not': 0.125, 'treated': 0.125, 'enlisted': 0.125, 'life,': 0.125, 'created': 0.125, 'truly': 0.125}, 'created': {'equal,': 1.0}, 'equal,': {'that': 0.5, 'then': 0.5}, 'they': {'are': 1.0}, 'endowed': {'by': 1.0}, 'by': {'their': 1.0}, 'their': {'creator': 0.3333333333333333, 'work;': 0.3333333333333333, 'efforts.': 0.3333333333333333}, 'creator': {'with': 1.0}, 'with': {'certain': 0.5, 'other': 0.5}, 'certain': {'unalienable': 1.0}, 'unalienable': {'rights,': 1.0}, 'rights,': {'that': 1.0}, 'among': {'these': 1.0}, 'life,': {'liberty,': 1.0}, 'liberty,': {'and': 1.0}, 'and': {'the': 0.13333333333333333, 'pride': 0.06666666666666667, 'a': 0.06666666666666667, 'resolve': 0.06666666666666667, 'fear.': 0.06666666666666667, 'we': 0.06666666666666667, 'investing': 0.06666666666666667, 'engineers': 0.06666666666666667, 'rule': 0.06666666666666667, 'sisters': 0.06666666666666667, 'uphold': 0.06666666666666667, 'daughters': 0.06666666666666667, 'future': 0.06666666666666667, 'dignity.': 0.06666666666666667}, 'pursuit': {'of': 1.0}, 'happiness.': {'for': 1.0}, 'for': {'the': 0.2, 'if': 0.2, 'we,': 0.2, 'hours': 0.2, 'no': 0.2}, 'we,': {'the': 1.0}, 'people,': {'still': 0.6666666666666666, 'understand': 0.3333333333333333}, 'understand': {'that': 1.0}, 'country': {'cannot': 0.5, 'and': 0.5}, 'cannot': {'succeed': 1.0}, 'succeed': {'when': 1.0}, 'when': {'a': 0.3333333333333333, 'the': 0.3333333333333333, 'every': 0.3333333333333333}, 'shrinking': {'few': 1.0}, 'few': {'do': 1.0}, 'do': {'very': 0.5, 'so': 0.5}, 'very': {'well': 1.0}, 'well': {'and': 1.0}, 'growing': {'many': 1.0}, 'many': {'barely': 1.0}, 'barely': {'make': 1.0}, 'make': {'the': 0.5, 'it.': 0.5}, 'it.': {'we': 1.0}, 'believe': {'that': 1.0}, 'america’s': {'prosperity': 1.0}, 'prosperity': {'must': 1.0}, 'must': {'make': 0.25, 'choose': 0.25, 'rest': 0.25, 'be': 0.25}, 'rest': {'upon': 1.0}, 'upon': {'the': 1.0}, 'broad': {'shoulders': 1.0}, 'shoulders': {'of': 1.0}, 'rising': {'middle': 1.0}, 'middle': {'class.': 1.0}, 'class.': {'we': 1.0}, 'know': {'that': 1.0}, 'america': {'must': 0.25, 'will': 0.25, 'as': 0.25, 'thrives': 0.25}, 'thrives': {'when': 1.0}, 'every': {'corner': 0.3333333333333333, 'person': 0.3333333333333333, 'citizen': 0.3333333333333333}, 'person': {'can': 1.0}, 'can': {'find': 0.3333333333333333, 'more': 0.3333333333333333, 'earn': 0.3333333333333333}, 'find': {'a': 0.5, 'independence': 0.5}, 'independence': {'and': 1.0}, 'pride': {'in': 1.0}, 'work;': {'when': 1.0}, 'wages': {'of': 1.0}, 'honest': {'labor': 1.0}, 'labor': {'liberate': 1.0}, 'liberate': {'families': 1.0}, 'families': {'from': 1.0}, 'from': {'the': 0.5, 'our': 0.5}, 'brink': {'of': 1.0}, 'hardship.': {'we,': 1.0}, 'still': {'see': 0.3333333333333333, 'believe': 0.6666666666666666}, 'citizen': {'deserves': 0.5, 'is': 0.5}, 'deserves': {'a': 1.0}, 'basic': {'measure': 1.0}, 'measure': {'of': 1.0}, 'security': {'and': 1.0}, 'dignity.': {'we': 1.0}, 'hard': {'choices': 1.0}, 'choices': {'to': 1.0}, 'reduce': {'the': 1.0}, 'cost': {'of': 1.0}, 'health': {'care': 1.0}, 'care': {'and': 1.0}, 'size': {'of': 1.0}, 'deficit.': {'but': 1.0}, 'but': {'to': 0.3333333333333333, 'we': 0.3333333333333333, 'because': 0.3333333333333333}, 'reject': {'the': 1.0}, 'belief': {'that': 1.0}, 'choose': {'between': 1.0}, 'between': {'caring': 1.0}, 'caring': {'for': 1.0}, 'generation': {'that': 1.0}, 'built': {'this': 1.0}, 'investing': {'in': 1.0}, 'will': {'defend': 0.16666666666666666, 'respond': 0.16666666666666666, 'show': 0.16666666666666666, 'remain': 0.16666666666666666, 'build': 0.16666666666666666, 'renew': 0.16666666666666666}, 'build': {'its': 1.0}, 'its': {'most': 0.5, 'future.': 0.5}, 'future.': {'we,': 1.0}, 'obligations': {'as': 1.0}, 'as': {'a': 0.3333333333333333, 'americans': 0.3333333333333333, 'well.': 0.3333333333333333}, 'americans': {'are': 1.0}, 'just': {'to': 1.0}, 'ourselves,': {'but': 1.0}, 'posterity.': {'we': 1.0}, 'respond': {'to': 1.0}, 'threat': {'of': 1.0}, 'climate': {'change,': 1.0}, 'change,': {'knowing': 1.0}, 'knowing': {'that': 1.0}, 'failure': {'to': 1.0}, 'so': {'would': 1.0}, 'would': {'betray': 1.0}, 'betray': {'our': 1.0}, 'children': {'and': 1.0}, 'future': {'generations.': 1.0}, 'generations.': {'we': 1.0}, 'defend': {'our': 1.0}, 'people': {'and': 1.0}, 'uphold': {'our': 1.0}, 'values': {'through': 1.0}, 'through': {'strength': 1.0}, 'arms': {'and': 1.0}, 'rule': {'of': 1.0}, 'law.': {'we': 1.0}, 'show': {'the': 1.0}, 'courage': {'to': 1.0}, 'try': {'and': 1.0}, 'resolve': {'our': 1.0}, 'differences': {'with': 1.0}, 'other': {'nations': 1.0}, 'nations': {'peacefully': 1.0}, 'peacefully': {'–': 1.0}, 'because': {'engagement': 0.5, 'we': 0.5}, 'naïve': {'about': 1.0}, 'about': {'the': 1.0}, 'dangers': {'we': 1.0}, 'face,': {'but': 1.0}, 'engagement': {'can': 1.0}, 'durably': {'lift': 1.0}, 'lift': {'suspicion': 1.0}, 'suspicion': {'and': 1.0}, 'fear.': {'america': 1.0}, 'remain': {'the': 1.0}, 'anchor': {'of': 1.0}, 'strong': {'alliances': 1.0}, 'alliances': {'in': 1.0}, 'corner': {'of': 1.0}, 'globe;': {'and': 1.0}, 'renew': {'those': 1.0}, 'those': {'institutions': 1.0}, 'institutions': {'that': 1.0}, 'extend': {'our': 1.0}, 'capacity': {'to': 1.0}, 'manage': {'crisis': 1.0}, 'crisis': {'abroad,': 1.0}, 'abroad,': {'for': 1.0}, 'no': {'one': 0.5, 'citizen': 0.5}, 'one': {'has': 0.5, 'another': 0.5}, 'has': {'a': 1.0}, 'greater': {'stake': 1.0}, 'stake': {'in': 1.0}, 'peaceful': {'world': 1.0}, 'world': {'than': 1.0}, 'most': {'powerful': 1.0}, 'powerful': {'nation.': 1.0}, 'nation.': {'our': 1.0}, 'journey': {'is': 1.0}, 'complete': {'until': 1.0}, 'until': {'bright': 0.2, 'we': 0.2, 'our': 0.4, 'no': 0.2}, 'wives,': {'our': 1.0}, 'mothers,': {'and': 1.0}, 'daughters': {'can': 1.0}, 'earn': {'a': 1.0}, 'living': {'equal': 1.0}, 'equal': {'to': 0.5, 'as': 0.5}, 'efforts.': {'our': 1.0}, 'gay': {'brothers': 1.0}, 'brothers': {'and': 1.0}, 'sisters': {'are': 1.0}, 'treated': {'like': 1.0}, 'like': {'anyone': 1.0}, 'anyone': {'else': 1.0}, 'else': {'under': 1.0}, 'under': {'the': 1.0}, 'law': {'–': 1.0}, 'if': {'we': 1.0}, 'truly': {'created': 1.0}, 'then': {'surely': 1.0}, 'surely': {'the': 1.0}, 'love': {'we': 1.0}, 'commit': {'to': 1.0}, 'another': {'must': 1.0}, 'well.': {'our': 1.0}, 'forced': {'to': 1.0}, 'wait': {'for': 1.0}, 'hours': {'to': 1.0}, 'exercise': {'the': 1.0}, 'right': {'to': 1.0}, 'vote.': {'our': 1.0}, 'better': {'way': 1.0}, 'way': {'to': 1.0}, 'welcome': {'the': 1.0}, 'striving,': {'hopeful': 1.0}, 'hopeful': {'immigrants': 1.0}, 'immigrants': {'who': 1.0}, 'who': {'still': 1.0}, 'see': {'america': 1.0}, 'land': {'of': 1.0}, 'opportunity;': {'until': 1.0}, 'bright': {'young': 1.0}, 'young': {'students': 1.0}, 'students': {'and': 1.0}, 'engineers': {'are': 1.0}, 'enlisted': {'in': 1.0}, 'workforce': {'rather': 1.0}, 'rather': {'than': 1.0}, 'expelled': {'from': 1.0}})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_conditional_probabilities(corpus):\n",
    "\t\"\"\"\n",
    "\tThe function takes as its input a corpus string (words separated by \n",
    "\tspaces) and returns a 2D dictionnary of probabilities P(next|current) of\n",
    "\tseeing a word \"next\" conditionnaly to seeing a word \"current\". \n",
    "\t\"\"\"\n",
    "\n",
    "\t# First we parse the string to build a double dimension dictionnary that\n",
    "\t# returns the conditional probabilities.\n",
    "\n",
    "\t# We parse the string to build a first dictionnary indicating for each\n",
    "\t# word, what are the words that follow it in the string. Repeated next\n",
    "\t# words are kept so we use a list and not a set. \n",
    "\n",
    "\ttokenized_string = corpus.split()\n",
    "\tprevious_word = \"\"\n",
    "\tdictionnary = defaultdict(list)\n",
    "\n",
    "\tfor current_word in tokenized_string:\n",
    "\t\tif previous_word != \"\":\n",
    "\t\t\tdictionnary[previous_word].append(current_word)\n",
    "\t\tprevious_word = current_word\n",
    "\t\t\n",
    "\t# We know parse dictionnary to compute the probability each observed\n",
    "\t# next word for each word in the dictionnary. \n",
    "\n",
    "\tfor key in dictionnary.keys():\n",
    "\t\tnext_words = dictionnary[key]\n",
    "\t\tunique_words = set(next_words) # removes duplicated\n",
    "\t\tnb_words = len(next_words)\n",
    "\t\tprobabilities_given_key = {}\n",
    "\t\tfor unique_word in unique_words:\n",
    "\t\t\tprobabilities_given_key[unique_word] = \\\n",
    "\t\t\t\tfloat(next_words.count(unique_word)) / nb_words\n",
    "\t\tdictionnary[key] = probabilities_given_key\n",
    "\n",
    "\treturn dictionnary\n",
    "\n",
    "\n",
    "def bigram_next_word_predictor(conditional_probabilities, current, next_candidate):\n",
    "\t\"\"\"\n",
    "\tThe function takes as its input a 2D dictionnary of probabilities \n",
    "\tP(next|current) of seeing a word \"next\" conditionnaly to seeing a word \n",
    "\t\"current\", the current word being read, and a next candidate word, and\n",
    "\treturns P(next_candidate|current).\n",
    "\t\"\"\"\n",
    "\n",
    "\t# We look for the probability corresponding to the \n",
    "\t# current -> next_candidate pair\n",
    "\n",
    "\tif current in conditional_probabilities:\n",
    "\t\tif next_candidate in conditional_probabilities[current]:\n",
    "\t\t\treturn conditional_probabilities[current][next_candidate]\n",
    "\n",
    "\t# If current -> next_candidate pair has not been observed in the corpus,\n",
    "\t# the corresponding dictionnary keys will not be defined. We return \n",
    "\t# a probability 0.0\n",
    "\n",
    "\treturn 0.0\n",
    "\n",
    "# An example corpus to try out the function\n",
    "corpus = \"Each time we gather to inaugurate a president, we bear witness to the enduring strength of our Constitution. We affirm the promise of our democracy. We recall that what binds this nation together is not the colors of our skin or the tenets of our faith or the origins of our names. What makes us exceptional – what makes us American – is our allegiance to an idea, articulated in a declaration made more than two centuries ago:    “We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable rights, that among these are Life, Liberty, and the pursuit of Happiness. For we, the people, understand that our country cannot succeed when a shrinking few do very well and a growing many barely make it. We believe that America’s prosperity must rest upon the broad shoulders of a rising middle class. We know that America thrives when every person can find independence and pride in their work; when the wages of honest labor liberate families from the brink of hardship. We, the people, still believe that every citizen deserves a basic measure of security and dignity. We must make the hard choices to reduce the cost of health care and the size of our deficit. But we reject the belief that America must choose between caring for the generation that built this country and investing in the generation that will build its future. We, the people, still believe that our obligations as Americans are not just to ourselves, but to all posterity. We will respond to the threat of climate change, knowing that the failure to do so would betray our children and future generations. We will defend our people and uphold our values through strength of arms and rule of law. We will show the courage to try and resolve our differences with other nations peacefully – not because we are naïve about the dangers we face, but because engagement can more durably lift suspicion and fear. America will remain the anchor of strong alliances in every corner of the globe; and we will renew those institutions that extend our capacity to manage crisis abroad, for no one has a greater stake in a peaceful world than its most powerful nation. Our journey is not complete until our wives, our mothers, and daughters can earn a living equal to their efforts. Our journey is not complete until our gay brothers and sisters are treated like anyone else under the law – for if we are truly created equal, then surely the love we commit to one another must be equal as well. Our journey is not complete until no citizen is forced to wait for hours to exercise the right to vote.  Our journey is not complete until we find a better way to welcome the striving, hopeful immigrants who still see America as a land of opportunity; until bright young students and engineers are enlisted in our workforce rather than expelled from our country.\"\n",
    "corpus = corpus.lower()\n",
    "# We call the conditional probability dictionnary builder function\n",
    "conditional_probabilities = build_conditional_probabilities(corpus)\n",
    "\n",
    "# Some sample queries to the bigram predictor\n",
    "assert bigram_next_word_predictor(conditional_probabilities, \"our\", \"people\") \n",
    "assert bigram_next_word_predictor(conditional_probabilities, \"our\", \"journey\")\n",
    "#assert bigram_next_word_predictor(conditional_probabilities, \"\", \"red\") == 0.0\n",
    "\n",
    "print(conditional_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert bigram_next_word_predictor(conditional_probabilities, \"our\", \"people\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/kenmckee/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "254989\n",
      "['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']', 'etymology', '.', '(', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school', ')', 'the', 'pale', 'usher', '--', 'threadbare', 'in', 'coat', ',', 'heart', ',', 'body', ',', 'and', 'brain', ';', 'i', 'see', 'him', 'now', '.', 'he', 'was', 'ever', 'dusting', 'his', 'old', 'lexicons', 'and', 'grammars', ',', 'with', 'a', 'queer', 'handkerchief', ',', 'mockingly', 'embellished', 'with', 'all', 'the', 'gay', 'flags', 'of', 'all', 'the', 'known', 'nations', 'of', 'the', 'world', '.', 'he', 'loved', 'to', 'dust', 'his', 'old', 'grammars', ';', 'it', 'somehow', 'mildly', 'reminded', 'him', 'of', 'his', 'mortality', '.', '``', 'while', 'you', 'take', 'in', 'hand', 'to', 'school', 'others', ',', 'and', 'to', 'teach', 'them', 'by', 'what', 'name', 'a', 'whale-fish', 'is']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "\n",
    "nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "\n",
    "file0 = nltk.corpus.gutenberg.fileids()[12]\n",
    "emmatext = nltk.corpus.gutenberg.raw(file0)\n",
    "emmatokens = nltk.word_tokenize(emmatext) \n",
    "emmawords = [w.lower( ) for w in emmatokens]\n",
    "\n",
    "print(len(emmawords))\n",
    "print(emmawords[ :110])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", \t 19204\n",
      "the \t 14416\n",
      ". \t 7308\n",
      "of \t 6586\n",
      "and \t 6414\n",
      "a \t 4694\n",
      "to \t 4597\n",
      "; \t 4173\n",
      "in \t 4162\n",
      "that \t 3080\n",
      "his \t 2530\n",
      "it \t 2507\n",
      "i \t 2097\n",
      "he \t 1890\n",
      "but \t 1813\n",
      "! \t 1767\n",
      "is \t 1748\n",
      "as \t 1741\n",
      "with \t 1721\n",
      "-- \t 1713\n",
      "was \t 1651\n",
      "'s \t 1634\n",
      "for \t 1616\n",
      "'' \t 1615\n",
      "all \t 1508\n",
      "`` \t 1456\n",
      "this \t 1391\n",
      "at \t 1318\n",
      "not \t 1218\n",
      "by \t 1201\n"
     ]
    }
   ],
   "source": [
    "ndist = FreqDist(emmawords)\n",
    "nitems = ndist.most_common(30)\n",
    "for item in nitems:\n",
    "    print (item[0], '\\t', item[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']', 'etymology', '.', '(', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school', ')', 'the', 'pale', 'usher', '--', 'threadbare', 'in', 'coat', ',', 'heart', ',', 'body', ',', 'and', 'brain', ';', 'i', 'see', 'him', 'now', '.', 'he', 'was', 'ever', 'dusting', 'his', 'old', 'lexicons', 'and', 'grammars', ',', 'with', 'a', 'queer', 'handkerchief', ',', 'mockingly', 'embellished', 'with', 'all', 'the', 'gay', 'flags', 'of', 'all', 'the', 'known', 'nations', 'of', 'the', 'world', '.', 'he', 'loved', 'to', 'dust', 'his', 'old', 'grammars', ';', 'it', 'somehow', 'mildly', 'reminded', 'him', 'of', 'his', 'mortality', '.', '``', 'while', 'you', 'take', 'in', 'hand', 'to', 'school', 'others', ',', 'and', 'to', 'teach', 'them', 'by', 'what', 'name', 'a', 'whale-fish', 'is', 'to', 'be', 'called', 'in', 'our', 'tongue', 'leaving', 'out', ',', 'through', 'ignorance', ',', 'the', 'letter', 'h', ',', 'which', 'almost', 'alone', 'maketh', 'the', 'signification', 'of', 'the', 'word', ',', 'you', 'deliver', 'that', 'which', 'is', 'not', 'true', '.', \"''\", '--', 'hackluyt', \"''\", 'whale', '.', '...', 'sw.', 'and', 'dan', '.', 'hval', '.', 'this', 'animal', 'is']\n",
      "['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']', 'etymology', '.', '(', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school', ')', 'the', 'pale', 'usher', '--', 'threadbare', 'in', 'coat', ',', 'heart', ',', 'body', ',', 'and', 'brain', ';', 'i', 'see', 'him', 'now', '.', 'he', 'was', 'ever', 'dusting', 'his', 'old', 'lexicons', 'and', 'grammars', ',', 'with', 'a', 'queer', 'handkerchief', ',', 'mockingly', 'embellished', 'with', 'all', 'the', 'gay', 'flags', 'of', 'all', 'the', 'known', 'nations', 'of', 'the', 'world', '.', 'he', 'loved', 'to', 'dust', 'his', 'old', 'grammars', ';', 'it', 'somehow', 'mildly', 'reminded', 'him', 'of', 'his', 'mortality', '.', '\"', 'while', 'you', 'take', 'in', 'hand', 'to', 'school', 'others', ',', 'and', 'to', 'teach', 'them', 'by', 'what', 'name', 'a', 'whale', '-', 'fish', 'is', 'to', 'be', 'called', 'in', 'our', 'tongue', 'leaving', 'out', ',', 'through', 'ignorance', ',', 'the', 'letter', 'h', ',', 'which', 'almost', 'alone', 'maketh', 'the', 'signification', 'of', 'the', 'word', ',', 'you', 'deliver', 'that', 'which', 'is', 'not', 'true', '.\"', '--', 'hackluyt', '\"', 'whale', '.', '...', 'sw', '.', 'and', 'dan', '.', 'hval', '.', 'this']\n"
     ]
    }
   ],
   "source": [
    "emmawords2 = gutenberg.words('melville-moby_dick.txt')\n",
    "emmawords2lowercase = [w.lower() for w in emmawords2]\n",
    "\n",
    "len(emmawords)\n",
    "len(emmawords2lowercase)\n",
    "\n",
    "print(emmawords[:160])\n",
    "print(emmawords2lowercase[:160])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'32-16'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> emptydict = dict()\n",
    ">>> phonedict = {'Bailey':'32-16','Char':'15-18', 'Dave': '20-15'}\n",
    "\n",
    ">>> phonedict['Bailey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bailey': '32-16', 'Char': '15-18', 'Dave': '20-15', 'Avi': '41-54'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> phonedict['Avi'] = '41-54'\n",
    ">>> phonedict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Bailey', 'Char', 'Dave', 'Avi'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> phonedict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['32-16', '15-18', '20-15', '41-54'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> phonedict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Bailey', '32-16'), ('Char', '15-18'), ('Dave', '20-15'), ('Avi', '41-54')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> phonedict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> 'Char' in phonedict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> 'Dave' not in phonedict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bailey', '32-16')\n",
      "('Char', '15-18')\n",
      "('Dave', '20-15')\n",
      "('Avi', '41-54')\n"
     ]
    }
   ],
   "source": [
    ">>> for pair in phonedict.items():\n",
    "           print(pair)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function doublesum takes 2 numbers as parameters, either int or float\n",
    "#  and returns a result which is the sum of those numbers multiplied by 2\n",
    "def doublesum (x, y):\n",
    "    result = 2 * (x + y)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> doublesum(3, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> num = doublesum(3.4, 2)\n",
    ">>> num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes a string and a list of words as parameters.\n",
    "#   It will return all the words in the list that contain the string as a substring\n",
    "def searchstring (substring, wordlist):\n",
    "    # initialize the result\n",
    "    result = [ ]\n",
    "    #  loop over all the words\n",
    "    for word in wordlist:\n",
    "        # test each word if it contains the substring\n",
    "        if substring in word:\n",
    "            # add it to the result\n",
    "            result.append(word)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fuzzing',\n",
       " 'drizzly',\n",
       " 'puzzled',\n",
       " 'dazzling',\n",
       " 'puzzle',\n",
       " 'puzzled',\n",
       " 'puzzled',\n",
       " 'mizzen',\n",
       " 'huzza',\n",
       " 'huzza',\n",
       " 'huzza',\n",
       " 'huzza',\n",
       " 'huzza',\n",
       " 'belshazzar',\n",
       " 'belshazzar',\n",
       " 'belshazzar',\n",
       " 'belshazzar',\n",
       " 'huzza',\n",
       " 'huzza',\n",
       " 'dazzlingly',\n",
       " 'mizzen',\n",
       " 'piazza',\n",
       " 'plazza',\n",
       " 'mizzen',\n",
       " 'whizzings',\n",
       " 'mizzen-mast-heads',\n",
       " 'gizzard',\n",
       " 'grizzled',\n",
       " 'puzzling',\n",
       " 'muezzin',\n",
       " 'muzzle',\n",
       " 'grizzled',\n",
       " 'puzzling',\n",
       " 'piazza',\n",
       " 'belshazzar',\n",
       " 'grizzled',\n",
       " 'puzzle',\n",
       " 'huzza',\n",
       " 'puzzle',\n",
       " 'grizzly',\n",
       " 'dazzling',\n",
       " 'dazzlingly',\n",
       " 'dazzling']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> searchstring('zz', emmawords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zack'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiple variable assignment and use\n",
    ">>> name, phone, location = ('Zack', '22-15', 'Room 159')\n",
    ">>> name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'22-15'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> phone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Room 159'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# this regular expression pattern matches any word that contains all non-alphabetical\n",
    "#   lower-case characters [^a-z]+\n",
    "# the beginning ^ and ending $ require the match to begin and end on a word boundary \n",
    "pattern = re.compile('^[^a-z]+$')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonAlphaMatch = pattern.match('**')\n",
    "#  if it matched, print a message\n",
    "if nonAlphaMatch: 'matched non-alphabetical'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a word and returns true if it consists only\n",
    "#   of non-alphabetic characters\n",
    "\n",
    "def alpha_filter(w):\n",
    "  # pattern to match a word of non-alphabetical characters\n",
    "    pattern = re.compile('^[^a-z]+$')\n",
    "    if (pattern.match(w)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215607\n",
      "['moby', 'dick', 'by', 'herman', 'melville', 'etymology', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school', 'the', 'pale', 'usher', 'threadbare', 'in', 'coat', 'heart', 'body', 'and', 'brain', 'i', 'see', 'him', 'now', 'he', 'was', 'ever', 'dusting', 'his', 'old', 'lexicons', 'and', 'grammars', 'with', 'a', 'queer', 'handkerchief', 'mockingly', 'embellished', 'with', 'all', 'the', 'gay', 'flags', 'of', 'all', 'the', 'known', 'nations', 'of', 'the', 'world', 'he', 'loved', 'to', 'dust', 'his', 'old', 'grammars', 'it', 'somehow', 'mildly', 'reminded', 'him', 'of', 'his', 'mortality', 'while', 'you', 'take', 'in', 'hand', 'to', 'school', 'others', 'and', 'to', 'teach', 'them', 'by', 'what', 'name', 'a', 'whale-fish', 'is', 'to', 'be', 'called', 'in', 'our', 'tongue', 'leaving', 'out', 'through']\n"
     ]
    }
   ],
   "source": [
    "alphaemmawords = [w for w in emmawords if not alpha_filter(w)]\n",
    "print(len(alphaemmawords))\n",
    "print(alphaemmawords[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "print(len(nltkstopwords))\n",
    "print(nltkstopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']', 'etymology', '.', '(', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school', ')', 'the', 'pale', 'usher', '--', 'threadbare', 'in', 'coat', ',', 'heart', ',', 'body', ',', 'and', 'brain', ';', 'i', 'see', 'him', 'now', '.', 'he', 'was', 'ever', 'dusting', 'his', 'old', 'lexicons', 'and', 'grammars', ',', 'with', 'a', 'queer', 'handkerchief', ',', 'mockingly', 'embellished', 'with', 'all', 'the', 'gay', 'flags', 'of', 'all', 'the', 'known', 'nations', 'of', 'the', 'world', '.', 'he', 'loved', 'to', 'dust', 'his', 'old', 'grammars', ';', 'it', 'somehow', 'mildly', 'reminded', 'him', 'of', 'his', 'mortality', '.', '``', 'while', 'you', 'take', 'in', 'hand', 'to', 'school', 'others', ',']\n",
      "['who-e', 'debel', 'you', '?', \"''\", '--', 'he', 'at', 'last', 'said']\n"
     ]
    }
   ],
   "source": [
    "print(emmawords[:100])\n",
    "print(emmawords[15300:15310])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "morestopwords = ['could','would','might','must','need','sha','wo','y',\"'s\",\"'d\",\"'ll\",\"'t\",\"'m\",\"'re\",\"'ve\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'could', 'would', 'might', 'must', 'need', 'sha', 'wo', 'y', \"'s\", \"'d\", \"'ll\", \"'t\", \"'m\", \"'re\", \"'ve\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltkstopwords + morestopwords\n",
    "print(len(stopwords))\n",
    "print(stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107330\n"
     ]
    }
   ],
   "source": [
    "stoppedemmawords = [w for w in alphaemmawords if not w in stopwords]\n",
    "print(len(stoppedemmawords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('whale', 1086)\n",
      "('one', 912)\n",
      "('like', 580)\n",
      "('upon', 565)\n",
      "('ahab', 508)\n",
      "('man', 490)\n",
      "('ship', 463)\n",
      "('old', 443)\n",
      "('ye', 438)\n",
      "('sea', 384)\n",
      "('though', 382)\n",
      "('yet', 344)\n",
      "('time', 326)\n",
      "('captain', 324)\n",
      "('long', 318)\n",
      "('still', 312)\n",
      "('said', 304)\n",
      "('great', 303)\n",
      "('two', 288)\n",
      "('boat', 287)\n",
      "('seemed', 283)\n",
      "('head', 277)\n",
      "('last', 275)\n",
      "('see', 268)\n",
      "('thou', 268)\n",
      "('whales', 267)\n",
      "('way', 264)\n",
      "('stubb', 254)\n",
      "(\"n't\", 252)\n",
      "('queequeg', 252)\n"
     ]
    }
   ],
   "source": [
    "emmadist = FreqDist(stoppedemmawords)\n",
    "emmaitems = emmadist.most_common(30)\n",
    "for item in emmaitems:\n",
    "  print(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'moby'), ('moby', 'dick'), ('dick', 'by'), ('by', 'herman'), ('herman', 'melville'), ('melville', '1851'), ('1851', ']'), (']', 'etymology'), ('etymology', '.'), ('.', '('), ('(', 'supplied'), ('supplied', 'by'), ('by', 'a'), ('a', 'late'), ('late', 'consumptive'), ('consumptive', 'usher'), ('usher', 'to'), ('to', 'a'), ('a', 'grammar'), ('grammar', 'school')]\n"
     ]
    }
   ],
   "source": [
    "emmabigrams = list(nltk.bigrams(emmawords))\n",
    "\n",
    "print(emmabigrams[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(emmawords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'tuple'> ((',', 'and'), 0.010357309530999377)\n"
     ]
    }
   ],
   "source": [
    "print(type(scored))\n",
    "first = scored[0]\n",
    "print(type(first), first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((',', 'and'), 0.010357309530999377)\n",
      "(('of', 'the'), 0.0073493366380510535)\n",
      "(('in', 'the'), 0.004604120177733157)\n",
      "((',', 'the'), 0.003600155300816898)\n",
      "((';', 'and'), 0.0033609292949891957)\n",
      "(('to', 'the'), 0.002851103380930158)\n",
      "(('.', '``'), 0.002337355729070666)\n",
      "(('.', 'but'), 0.002337355729070666)\n",
      "((',', 'that'), 0.0023059818266670325)\n",
      "((',', 'as'), 0.002058912345238422)\n",
      "(('.', \"''\"), 0.001917729784422073)\n",
      "((\"''\", '``'), 0.001854981979614807)\n",
      "((',', 'i'), 0.0017961559126079948)\n",
      "((',', 'he'), 0.0017687037480048158)\n",
      "(('from', 'the'), 0.0017255646321998204)\n",
      "((',', 'in'), 0.0015804603335830172)\n",
      "(('.', 'the'), 0.0014981038397734803)\n",
      "(('of', 'his'), 0.001458886461768939)\n",
      "(('and', 'the'), 0.0014431995105671225)\n",
      "(('the', 'whale'), 0.0014039821325625812)\n",
      "(('on', 'the'), 0.0013686864923584939)\n",
      "((',', 'but'), 0.001345156065555769)\n",
      "((';', 'but'), 0.0013373125899548608)\n",
      "(('of', 'a'), 0.0013059386875512277)\n",
      "(('at', 'the'), 0.0012902517363494112)\n",
      "(('to', 'be'), 0.0012902517363494112)\n",
      "(('!', \"''\"), 0.0012471126205444156)\n",
      "((',', \"''\"), 0.0012471126205444156)\n",
      "(('by', 'the'), 0.0012471126205444156)\n",
      "(('with', 'the'), 0.0012235821937416908)\n"
     ]
    }
   ],
   "source": [
    "for bscore in scored[:30]:\n",
    "    print (bscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('of', 'the'), 0.0073493366380510535)\n",
      "(('in', 'the'), 0.004604120177733157)\n",
      "(('to', 'the'), 0.002851103380930158)\n",
      "(('from', 'the'), 0.0017255646321998204)\n",
      "(('of', 'his'), 0.001458886461768939)\n",
      "(('and', 'the'), 0.0014431995105671225)\n",
      "(('the', 'whale'), 0.0014039821325625812)\n",
      "(('on', 'the'), 0.0013686864923584939)\n",
      "(('of', 'a'), 0.0013059386875512277)\n",
      "(('at', 'the'), 0.0012902517363494112)\n",
      "(('to', 'be'), 0.0012902517363494112)\n",
      "(('by', 'the'), 0.0012471126205444156)\n",
      "(('with', 'the'), 0.0012235821937416908)\n",
      "(('for', 'the'), 0.0011961300291385118)\n",
      "(('it', 'was'), 0.0011529909133335162)\n",
      "(('it', 'is'), 0.001109851797528521)\n",
      "(('in', 'his'), 0.001027495303718984)\n",
      "(('in', 'a'), 0.0010118083525171675)\n",
      "(('with', 'a'), 0.000972590974512626)\n",
      "(('the', 'ship'), 0.0009647474989117178)\n",
      "(('into', 'the'), 0.0009608257611112636)\n",
      "(('upon', 'the'), 0.0008627823160999102)\n",
      "(('as', 'the'), 0.0008510171026985478)\n",
      "(('that', 'the'), 0.0008431736270976395)\n",
      "(('the', 'sea'), 0.0008431736270976395)\n",
      "(('all', 'the'), 0.0007921910356917357)\n",
      "(('out', 'of'), 0.0006823823772790199)\n",
      "(('sperm', 'whale'), 0.0006784606394785658)\n",
      "(('for', 'a'), 0.0006627736882767491)\n",
      "(('the', 'same'), 0.0006392432614740243)\n"
     ]
    }
   ],
   "source": [
    "finder.apply_word_filter(alpha_filter)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:30]:\n",
    "    print (bscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('sperm', 'whale'), 0.0006784606394785658)\n",
      "(('white', 'whale'), 0.00041570420684813853)\n",
      "(('moby', 'dick'), 0.0003176607618367851)\n",
      "(('old', 'man'), 0.0002941303350340603)\n",
      "(('captain', 'ahab'), 0.00023922600582770238)\n",
      "(('right', 'whale'), 0.00020393036562361513)\n",
      "(('captain', 'peleg'), 0.0001254956096145324)\n",
      "(('cried', 'ahab'), 0.0001254956096145324)\n",
      "(('mr.', 'starbuck'), 0.00011373039621316998)\n",
      "(('one', 'hand'), 0.00010980865841271584)\n",
      "(('let', 'us'), 0.00010588692061226171)\n",
      "(('ca', \"n't\"), 0.00010196518281180757)\n",
      "(('every', 'one'), 9.412170721089929e-05)\n",
      "(('cried', 'stubb'), 9.019996941044515e-05)\n",
      "(('look', 'ye'), 8.627823160999102e-05)\n",
      "(('never', 'mind'), 8.627823160999102e-05)\n",
      "(('one', 'side'), 8.627823160999102e-05)\n",
      "((\"'ye\", 'see'), 8.235649380953688e-05)\n",
      "(('thou', 'art'), 8.235649380953688e-05)\n",
      "(('ai', \"n't\"), 7.451301820862861e-05)\n",
      "(('new', 'bedford'), 7.059128040817447e-05)\n",
      "(('said', 'stubb'), 7.059128040817447e-05)\n",
      "(('sperm', 'whales'), 7.059128040817447e-05)\n",
      "(('years', 'ago'), 7.059128040817447e-05)\n",
      "(('cried', 'starbuck'), 6.666954260772033e-05)\n",
      "((\"n't\", 'know'), 6.666954260772033e-05)\n",
      "(('cape', 'horn'), 6.27478048072662e-05)\n",
      "(('greenland', 'whale'), 6.27478048072662e-05)\n",
      "(('lower', 'jaw'), 6.27478048072662e-05)\n",
      "((\"n't\", 'ye'), 6.27478048072662e-05)\n"
     ]
    }
   ],
   "source": [
    "finder.apply_word_filter(lambda w: w in stopwords)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:30]:\n",
    "    print (bscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((',', 'and'), 0.010357309530999377)\n",
      "(('of', 'the'), 0.0073493366380510535)\n",
      "(('in', 'the'), 0.004604120177733157)\n",
      "((',', 'the'), 0.003600155300816898)\n",
      "((';', 'and'), 0.0033609292949891957)\n",
      "(('to', 'the'), 0.002851103380930158)\n",
      "(('.', '``'), 0.002337355729070666)\n",
      "(('.', 'but'), 0.002337355729070666)\n",
      "((',', 'that'), 0.0023059818266670325)\n",
      "((',', 'as'), 0.002058912345238422)\n",
      "(('.', \"''\"), 0.001917729784422073)\n",
      "((\"''\", '``'), 0.001854981979614807)\n",
      "((',', 'i'), 0.0017961559126079948)\n",
      "((',', 'he'), 0.0017687037480048158)\n",
      "(('from', 'the'), 0.0017255646321998204)\n",
      "((',', 'in'), 0.0015804603335830172)\n",
      "(('.', 'the'), 0.0014981038397734803)\n",
      "(('of', 'his'), 0.001458886461768939)\n",
      "(('and', 'the'), 0.0014431995105671225)\n",
      "(('the', 'whale'), 0.0014039821325625812)\n",
      "(('of', 'the'), 0.0073493366380510535)\n",
      "(('in', 'the'), 0.004604120177733157)\n",
      "(('to', 'the'), 0.002851103380930158)\n",
      "((\"''\", '``'), 0.001854981979614807)\n",
      "(('from', 'the'), 0.0017255646321998204)\n",
      "(('of', 'his'), 0.001458886461768939)\n",
      "(('and', 'the'), 0.0014431995105671225)\n",
      "(('the', 'whale'), 0.0014039821325625812)\n",
      "(('on', 'the'), 0.0013686864923584939)\n",
      "(('of', 'a'), 0.0013059386875512277)\n",
      "(('at', 'the'), 0.0012902517363494112)\n",
      "(('to', 'be'), 0.0012902517363494112)\n",
      "(('by', 'the'), 0.0012471126205444156)\n",
      "(('with', 'the'), 0.0012235821937416908)\n",
      "(('for', 'the'), 0.0011961300291385118)\n",
      "(('it', 'was'), 0.0011529909133335162)\n",
      "(('it', 'is'), 0.001109851797528521)\n",
      "(('in', 'his'), 0.001027495303718984)\n",
      "(('in', 'a'), 0.0010118083525171675)\n",
      "(('with', 'a'), 0.000972590974512626)\n"
     ]
    }
   ],
   "source": [
    "finder2 = BigramCollocationFinder.from_words(emmawords)\n",
    "finder2.apply_freq_filter(2)\n",
    "scored = finder2.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:20]:\n",
    "    print (bscore)\n",
    "\n",
    "finder2.apply_ngram_filter(lambda w1, w2: len(w1) < 2)\n",
    "scored = finder2.score_ngrams(bigram_measures.raw_freq)\n",
    "for bscore in scored[:20]:\n",
    "    print (bscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('*in', 'sperm-whalemen'), 17.96007548627488)\n",
      "(('11', 'nightgown'), 17.96007548627488)\n",
      "(('12', 'biographical'), 17.96007548627488)\n",
      "(('121', 'midnight.'), 17.96007548627488)\n",
      "(('2,800', 'firkins'), 17.96007548627488)\n",
      "(('25', 'postscript'), 17.96007548627488)\n",
      "(('a.s.', 'walw-ian'), 17.96007548627488)\n",
      "(('accidental', 'advantages'), 17.96007548627488)\n",
      "(('adoring', 'cherubim'), 17.96007548627488)\n",
      "(('agassiz', 'imagines'), 17.96007548627488)\n",
      "(('agrarian', 'freebooting'), 17.96007548627488)\n",
      "(('air-freighted', 'demijohn'), 17.96007548627488)\n",
      "(('albert', 'durer'), 17.96007548627488)\n",
      "(('all-ramifying', 'heartlessness'), 17.96007548627488)\n",
      "(('amphitheatrical', 'heights'), 17.96007548627488)\n",
      "(('anacharsis', 'clootz'), 17.96007548627488)\n",
      "(('andrew', 'jackson'), 17.96007548627488)\n",
      "(('anno', '1652'), 17.96007548627488)\n",
      "(('annus', 'mirabilis'), 17.96007548627488)\n",
      "(('arkansas', 'duellist'), 17.96007548627488)\n",
      "(('aroostook', 'hemlock'), 17.96007548627488)\n",
      "(('arrantest', 'topers'), 17.96007548627488)\n",
      "(('asphaltic', 'pavement'), 17.96007548627488)\n",
      "(('atrocious', 'scoundrel'), 17.96007548627488)\n",
      "(('australian', 'settlement'), 17.96007548627488)\n",
      "(('baleful', 'comets'), 17.96007548627488)\n",
      "(('balena', 'vero'), 17.96007548627488)\n",
      "(('baliene', 'ordinaire'), 17.96007548627488)\n",
      "(('balmy', 'autumnal'), 17.96007548627488)\n",
      "(('baptizo', 'te'), 17.96007548627488)\n"
     ]
    }
   ],
   "source": [
    "finder3 = BigramCollocationFinder.from_words(emmawords)\n",
    "scored = finder3.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in scored[:30]:\n",
    "    print (bscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('samuel', 'enderby'), 14.278251446301137)\n",
      "(('mrs.', 'hussey'), 13.872612645024544)\n",
      "(('heidelburgh', 'tun'), 13.737683064938436)\n",
      "(('don', 'sebastian'), 13.468222389945208)\n",
      "(('st.', 'george'), 13.348640774192532)\n",
      "(('father', 'mapple'), 13.259635768133789)\n",
      "(('huzza', 'porpoise'), 13.034076067718662)\n",
      "(('d', \"'ye\"), 12.915681366916429)\n",
      "(('fiery', 'pit'), 12.889686158383487)\n",
      "(('steering', 'oar'), 12.375112985553727)\n",
      "(('cape', 'horn'), 11.932169489704997)\n",
      "(('seven', 'hundred'), 11.901181797221312)\n",
      "(('centuries', 'ago'), 11.79015048483257)\n",
      "(('moby', 'dick'), 11.58482147992334)\n",
      "(('new', 'york'), 11.55068455013718)\n",
      "(('new', 'zealand'), 11.55068455013718)\n",
      "(('new', 'bedford'), 11.550684550137179)\n",
      "(('book', 'ii'), 11.12718547211014)\n",
      "(('saturday', 'night'), 10.904793050773694)\n",
      "(('she', 'blows'), 10.716149903388795)\n",
      "(('drew', 'nigh'), 10.659341613656645)\n",
      "(('chief', 'mate'), 10.643309096503298)\n",
      "(('years', 'ago'), 10.452280846076183)\n",
      "(('english', 'whalers'), 10.419366223603452)\n",
      "(('forty', 'years'), 10.393387157022618)\n",
      "(('brought', 'alongside'), 10.279188565555193)\n",
      "(('(', 'sneezes'), 10.211882636685422)\n",
      "(('sneezes', ')'), 10.211882636685422)\n",
      "(('lower', 'jaw'), 10.205187984111411)\n",
      "(('four', 'oceans'), 10.194540739911908)\n"
     ]
    }
   ],
   "source": [
    "finder3.apply_freq_filter(5)\n",
    "scored = finder3.score_ngrams(bigram_measures.pmi)\n",
    "for bscore in scored[:30]:\n",
    "    print (bscore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
