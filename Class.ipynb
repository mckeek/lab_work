{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage:  python classifyKaggle.py  <corpus directory path> <limit number>\n",
    "# open python and nltk packages needed for processing\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sentiment_read_subjectivity\n",
    "# initialize the positive, neutral and negative word lists\n",
    "#(positivelist, neutrallist, negativelist) \n",
    "#    = sentiment_read_subjectivity.read_three_types('SentimentLexicons/subjclueslen1-HLTEMNLP05.tff')\n",
    "\n",
    "import sentiment_read_LIWC_pos_neg_words\n",
    "# initialize positve and negative word prefix lists from LIWC \n",
    "#   note there is another function isPresent to test if a word's prefix is in the list\n",
    "(poslist, neglist) = sentiment_read_LIWC_pos_neg_words.read_words()\n",
    "\n",
    "# define a feature definition function here\n",
    "# This feature function makes features for each document from the word_features variable\n",
    "#.   it returns a feature dictionary with Vocabulary keys and boolean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    '''\n",
    "    # This is \"proof of concept\" to show that you can call the LIWC isPresent function\n",
    "    #.  inside a feature function in order to detect positive or negative words\n",
    "    print('Test LIWC')\n",
    "    for word in document[:2]:   # looks at the first two words of each document\n",
    "        # tests if the word is in the positive emotion list from LIWC\n",
    "        if sentiment_read_LIWC_pos_neg_words.isPresent(word, poslist):\n",
    "            print(word, 'is positive')\n",
    "    '''\n",
    "    return features\n",
    "\n",
    "## cross-validation ##\n",
    "# this function takes the number of folds, the feature sets\n",
    "# it iterates over the folds, using different sections for training and testing in turn\n",
    "#   it prints the precision, recall and F score for each fold \n",
    "#.  (it does not compute the average over the folds)\n",
    "def cross_validation_PRF(num_folds, featuresets):\n",
    "    subset_size = int(len(featuresets)/num_folds)\n",
    "    print('Each fold size:', subset_size)\n",
    "    accuracy_list = []\n",
    "    # iterate over the folds\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[(i*subset_size):][:subset_size]\n",
    "        train_this_round = featuresets[:(i*subset_size)] + featuresets[((i+1)*subset_size):]\n",
    "        # train using train_this_round\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        # evaluate against test_this_round to produce the gold and predicted labels\n",
    "        goldlist = []\n",
    "        predictedlist = []\n",
    "        for (features, label) in test_this_round:\n",
    "            goldlist.append(label)\n",
    "            predictedlist.append(classifier.classify(features))\n",
    "\n",
    "        # call the function with our data\n",
    "        eval_measures(goldlist, predictedlist)\n",
    "    # this version doesn't save measures and compute averages\n",
    "\n",
    "# Function to compute precision, recall and F1 for each label\n",
    "#  and for any number of labels\n",
    "# Input: list of gold labels, list of predicted labels (in same order)\n",
    "# Output:  prints precision, recall and F1 for each label\n",
    "def eval_measures(gold, predicted):\n",
    "    # get a list of labels\n",
    "    labels = list(set(gold))\n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        recall = TP / (TP + FP)\n",
    "        precision = TP / (TP + FN)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('Label\\tPrecision\\tRecall\\t\\tF1')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))\n",
    "\n",
    "\n",
    "# function to read kaggle training file, train and test a classifier \n",
    "def processkaggle(dirPath,limitStr):\n",
    "  # convert the limit argument from a string to an int\n",
    "  limit = int(limitStr)\n",
    "  \n",
    "  os.chdir(dirPath)\n",
    "  \n",
    "  f = open('./train.tsv', 'r')\n",
    "  # loop over lines in the file and use the first limit of them\n",
    "  phrasedata = []\n",
    "  for line in f:\n",
    "    # ignore the first line starting with Phrase and read all lines\n",
    "    if (not line.startswith('Phrase')):\n",
    "      # remove final end of line character\n",
    "      line = line.strip()\n",
    "      # each line has 4 items separated by tabs\n",
    "      # ignore the phrase and sentence ids, and keep the phrase and sentiment\n",
    "      phrasedata.append(line.split('\\t')[2:4])\n",
    "  \n",
    "  # pick a random sample of length limit because of phrase overlapping sequences\n",
    "  random.shuffle(phrasedata)\n",
    "  phraselist = phrasedata[:limit]\n",
    "\n",
    "  print('Read', len(phrasedata), 'phrases, using', len(phraselist), 'random phrases')\n",
    "\n",
    "  #for phrase in phraselist[:10]:\n",
    "  #  print (phrase)\n",
    "  \n",
    "  # create list of phrase documents as (list of words, label)\n",
    "  phrasedocs = []\n",
    "  # add all the phrases\n",
    "  for phrase in phraselist:\n",
    "    tokens = nltk.word_tokenize(phrase[0])\n",
    "    phrasedocs.append((tokens, int(phrase[1])))\n",
    "  \n",
    "  # print a few\n",
    "  for phrase in phrasedocs[:10]:\n",
    "    print (phrase)\n",
    "\n",
    "  # possibly filter tokens\n",
    "\n",
    "  # continue as usual to get all words and create word features\n",
    "  all_words_list = [word for (sent,cat) in phrasedocs for word in sent]\n",
    "  all_words = nltk.FreqDist(all_words_list)\n",
    "  print(len(all_words))\n",
    "\n",
    "  # get the 1500 most frequently appearing keywords in the corpus\n",
    "  # note that you may want to vary this number for the size of the vocabulary\n",
    "  word_items = all_words.most_common(1500)\n",
    "  word_features = [word for (word,count) in word_items]\n",
    "\n",
    "  # feature sets from a feature definition function\n",
    "  featuresets = [(document_features(d, word_features), c) for (d, c) in phrasedocs]\n",
    "\n",
    "  # train classifier and show performance in cross-validation\n",
    "  num_folds = 3\n",
    "  cross_validation_PRF(num_folds, featuresets)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "commandline interface takes a directory name with kaggle subdirectory for train.tsv\n",
    "   and a limit to the number of kaggle phrases to use\n",
    "It then processes the files and trains a kaggle movie review sentiment classifier.\n",
    "\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    if (len(sys.argv) != 3):\n",
    "        print ('usage: classifyKaggle.py <corpus-dir> <limit>')\n",
    "        sys.exit(0)\n",
    "    processkaggle(sys.argv[1], sys.argv[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
