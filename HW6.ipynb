{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - build MNB with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to use the Sci-kit Learn (sklearn) package to build Multinomial Naive Bayes model, rank features, and use the model for prediction. \n",
    "\n",
    "The data from the Kaggle Sentiment Analysis on Movie Review Competition are used in this tutorial. Check out the details of the data and the competition on Kaggle.\n",
    "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews\n",
    "\n",
    "The tutorial also includes sample code to prepare your prediction result for submission to Kaggle. Although the competition is over, you can still submit your prediction to get an evaluation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as p \n",
    "train=p.read_csv(\"/Users/kenmckee/Desktop/GS/S18/tm/HW6/ddcfx.csv\", delimiter=',') \n",
    "y=train['sentiment'].values \n",
    "X=train['review'].values \n",
    "z=train['lie'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55,) (55,) (37,) (37,)\n",
      "['p' 'p' 'p' 'n' 'n' 'n' 'n' 'n' 'n' 'p' 'p' 'n' 'n' 'n' 'p' 'n' 'n' 'n'\n",
      " 'p' 'n' 'n' 'p' 'p' 'p' 'n' 'n' 'p' 'n' 'p' 'n' 'n' 'p' 'p' 'p' 'p' 'n'\n",
      " 'p' 'p' 'n' 'n' 'p' 'n' 'p' 'p' 'n' 'p' 'n' 'n' 'p' 'n' 'p' 'p' 'p' 'p'\n",
      " 'n']\n",
      "['t' 'f' 'f' 'f' 'f' 'f' 't' 't' 'f' 't' 'f' 'f' 't' 't' 'f' 'f' 't' 't'\n",
      " 'f' 'f' 't' 'f' 'f' 't' 'f' 't' 't' 'f' 'f' 'f' 't' 'f' 't' 't' 't' 'f'\n",
      " 't' 't' 't' 't' 'f' 't' 'f' 'f' 'f' 't' 't' 'f' 't' 'f' 't' 'f' 'f' 'f'\n",
      " 't']\n",
      "[\"'My sister and I ate at this restaurant called Matador. The overall look and ambiance of the restaurant was very appealing. We first ordered strawberry margaritas--which were really good.Then my sister ordered a spinach lasagna with Alfredo sauce and I ordered Pasta ravioli with marinara sauce. My sister and I unanimously agreed they were the best pastas we had ever had. It was a beautiful blend of flavors which complimented each other. I would totally recommend Matador and it was an overall amazing experience.'\"\n",
      " \"'The service is good and I just felt like home. Waitresses and waiters always ask me want to I need and how about the taste\"\n",
      " \"'Carlo\\\\'s Plate Shack was amazing! The waitress was friendly\"\n",
      " \"'The Seven Heaven restaurant was never known for a superior service but what we experienced last week was a disaster. The waiter would not notice us until we asked him 4 times to bring us the menu. The food was not exceptional either. It took them though 2 minutes to bring us a check after they spotted we finished eating and are not ordering more. Well\"\n",
      " \"'In each of the diner dish there are at least one fly in it. We are waiting for an hour for the dish cooked done. The taste reminds me of a smell that I never want to try any more\"]\n",
      "[\"'After I went shopping with some of my friend\"\n",
      " \"'This place used to be great. I can\\\\'t believe it\\\\'s current state. Instead of the cool\"\n",
      " \"'I recently ate at a restaurant called White Castle and it was a dine in. I had to wait 20 minutes before the waiter came to my table to take my order even though it was not busy. I had to wait another 30 minutes for my order to come. I had ordered a veggie burger with fries and Iced tea. The veggie patty was not properly cooked\"\n",
      " \"'I went there with two friends at 6pm. Long queue was there. But it didn\\\\'t take us long to wait. The waiter was nice but worked in a hurry. We ordered \\\\'Today\\\\'s Special\\\\'\"\n",
      " \"'Carlo\\\\'s Plate Shack was the worst dining experience of my life. Although my Southern Comfort Plate sounded to die for\"]\n",
      "[\"'After I went shopping with some of my friend\"\n",
      " \"'This place used to be great. I can\\\\'t believe it\\\\'s current state. Instead of the cool\"\n",
      " \"'I recently ate at a restaurant called White Castle and it was a dine in. I had to wait 20 minutes before the waiter came to my table to take my order even though it was not busy. I had to wait another 30 minutes for my order to come. I had ordered a veggie burger with fries and Iced tea. The veggie patty was not properly cooked\"\n",
      " \"'I went there with two friends at 6pm. Long queue was there. But it didn\\\\'t take us long to wait. The waiter was nice but worked in a hurry. We ordered \\\\'Today\\\\'s Special\\\\'\"\n",
      " \"'Carlo\\\\'s Plate Shack was the worst dining experience of my life. Although my Southern Comfort Plate sounded to die for\"]\n",
      "['n' 'n' 'n' 'n' 'n' 'n' 'p' 'p' 'p' 'p' 'n' 'n' 'n' 'p' 'n' 'p' 'n' 'p'\n",
      " 'p' 'n' 'n' 'p' 'n' 'n' 'p' 'p' 'p' 'p' 'p' 'p' 'n' 'p' 'p' 'n' 'n' 'p'\n",
      " 'p']\n",
      "['f' 't' 'f' 'f' 'f' 't' 'f' 't' 't' 'f' 't' 'f' 't' 't' 't' 'f' 'f' 't'\n",
      " 'f' 't' 't' 'f' 'f' 't' 't' 't' 'f' 't' 't' 't' 'f' 't' 't' 't' 'f' 'f'\n",
      " 'f']\n"
     ]
    }
   ],
   "source": [
    "# check the sklearn documentation for train_test_split\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "# \"test_size\" : float, int, None, optional\n",
    "# If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. \n",
    "# If int, represents the absolute number of test samples. \n",
    "# If None, the value is set to the complement of the train size. \n",
    "# By default, the value is set to 0.25. The default will change in version 0.21. It will remain 0.25 only if train_size is unspecified, otherwise it will complement the specified train_size.    \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xy_train, Xy_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "Xz_train, Xz_test, z_train, z_test = train_test_split(X, z, test_size=0.4, random_state=0)\n",
    "\n",
    "X_train = Xy_train\n",
    "X_test = Xy_test\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(y_train)\n",
    "print(z_train)\n",
    "print(X_train[0:5])\n",
    "print(X_test[0:5])\n",
    "print(X_test[0:5])\n",
    "print(y_test)\n",
    "print(z_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.1 Data Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'p', 'n'}\n",
      "[['n' 28]\n",
      " ['p' 27]]\n",
      "{'t', 'f'}\n",
      "[['f' 29]\n",
      " ['t' 26]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: `itemfreq` is deprecated!\n",
      "`itemfreq` is deprecated and will be removed in a future version. Use instead `np.unique(..., return_counts=True)`\n",
      "  if __name__ == '__main__':\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `itemfreq` is deprecated!\n",
      "`itemfreq` is deprecated and will be removed in a future version. Use instead `np.unique(..., return_counts=True)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Check how many training examples in each category\n",
    "# this is important to see whether the data set is balanced or skewed\n",
    "\n",
    "#print(list(y_train))\n",
    "\n",
    "training_labelsy = set(y_train)\n",
    "print(training_labelsy)\n",
    "from scipy.stats import itemfreq\n",
    "training_category_disty = itemfreq(y_train)\n",
    "print(training_category_disty)\n",
    "\n",
    "training_labelsz = set(z_train)\n",
    "print(training_labelsz)\n",
    "from scipy.stats import itemfreq\n",
    "training_category_distz = itemfreq(z_train)\n",
    "print(training_category_distz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'p', 'n'}\n",
      "[['n' 18]\n",
      " ['p' 19]]\n",
      "{'t', 'f'}\n",
      "[['f' 17]\n",
      " ['t' 20]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: `itemfreq` is deprecated!\n",
      "`itemfreq` is deprecated and will be removed in a future version. Use instead `np.unique(..., return_counts=True)`\n",
      "  if sys.path[0] == '':\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: DeprecationWarning: `itemfreq` is deprecated!\n",
      "`itemfreq` is deprecated and will be removed in a future version. Use instead `np.unique(..., return_counts=True)`\n"
     ]
    }
   ],
   "source": [
    "# Print out the category distribution in the test data set. \n",
    "#Is the test data set's category distribution similar to the training data set's?\n",
    "\n",
    "# Your code starts here\n",
    "#train=p.read_csv(\"/Users/kenmckee/Desktop/GS/S18/tm/ks/test.tsv\", delimiter='\\t')\n",
    "\n",
    "list(y_test)\n",
    "\n",
    "test_labelsy = set(y_test)\n",
    "print(test_labelsy)\n",
    "from scipy.stats import itemfreq\n",
    "test_category_disty = itemfreq(y_test)\n",
    "print(test_category_disty)\n",
    "\n",
    "list(z_test)\n",
    "\n",
    "test_labelsz = set(z_test)\n",
    "print(test_labelsz)\n",
    "from scipy.stats import itemfreq\n",
    "test_category_distz = itemfreq(z_test)\n",
    "print(test_category_distz)\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn contains two vectorizers\n",
    "\n",
    "# CountVectorizer can give you Boolean or TF vectors\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "# TfidfVectorizer can give you TF or TFIDF vectors\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "# Read the sklearn documentation to understand all vectorization options\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# several commonly used vectorizer setting\n",
    "\n",
    "#  unigram boolean vectorizer, set minimum document frequency to 5\n",
    "unigram_bool_vectorizer = CountVectorizer(encoding='latin-1', binary=True, min_df=1, stop_words='english')\n",
    "\n",
    "#  unigram term frequency vectorizer, set minimum document frequency to 5\n",
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False, min_df=1, stop_words='english')\n",
    "\n",
    "#  unigram and bigram term frequency vectorizer, set minimum document frequency to 5\n",
    "gram12_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(2,3), min_df=2, stop_words='english')\n",
    "\n",
    "#  unigram tfidf vectorizer, set minimum document frequency to 5\n",
    "unigram_tfidf_vectorizer = TfidfVectorizer(encoding='latin-1', use_idf=True, min_df=1, stop_words='english')\n",
    "\n",
    "VT = VarianceThreshold(threshold=.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: Vectorize the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk.stem\n",
    "\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([english_stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "stem_vectorizer = StemmedCountVectorizer(min_df=3, analyzer=\"word\", stop_words='english')\n",
    "X_train_stem = stem_vectorizer.fit_transform(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55, 40)\n",
      "[[1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 3 0 0 1 2 0 0 0 0 0 0\n",
      "  0 0 0 0]]\n",
      "40\n",
      "[('restaur', 29), ('call', 4), ('look', 22), ('order', 25), ('realli', 28), ('good', 15), ('best', 3), ('amaz', 0), ('experi', 9), ('servic', 31)]\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# check the content of a document vector\n",
    "print(X_train_stem.shape)\n",
    "print(X_train_stem[0].toarray())\n",
    "\n",
    "# check the size of the constructed vocabulary\n",
    "print(len(stem_vectorizer.vocabulary_))\n",
    "\n",
    "# print out the first 10 items in the vocabulary\n",
    "print(list(stem_vectorizer.vocabulary_.items())[:10])\n",
    "\n",
    "# check word index in vocabulary\n",
    "print(stem_vectorizer.vocabulary_.get('restaur'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55, 21)\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "(55, 321)\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.23916021 ... 0.23916021 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "check the size of the constructed vocabulary\n",
      "321\n",
      "21\n",
      "321\n",
      "[('sister', 245), ('ate', 16), ('restaurant', 225), ('called', 45), ('matador', 175), ('overall', 198), ('look', 164), ('ambiance', 7), ('appealing', 9), ('ordered', 195)]\n",
      "[('felt like', 5), ('food exceptional', 7), ('finished eating', 6), ('good place', 8), ('went xyz', 18), ('xyz restaurant', 20), ('went xyz restaurant', 19), ('went restaurant', 17), ('best experience', 2), ('marshall street', 10)]\n",
      "[('sister', 245), ('ate', 16), ('restaurant', 225), ('called', 45), ('matador', 175), ('overall', 198), ('look', 164), ('ambiance', 7), ('appealing', 9), ('ordered', 195)]\n",
      "225\n",
      "None\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "# The vectorizer can do \"fit\" and \"transform\"\n",
    "# fit is a process to collect unique tokens into the vocabulary\n",
    "# transform is a process to convert each document to vector based on the vocabulary\n",
    "# These two processes can be done together using fit_transform(), or used individually: fit() or transform()\n",
    "\n",
    "# fit vocabulary in training documents and transform the training documents into vectors\n",
    "X_train_tfidf = unigram_tfidf_vectorizer.fit_transform(X_train)\n",
    "X_train_bool = unigram_bool_vectorizer.fit_transform(X_train)\n",
    "X_train_count = unigram_count_vectorizer.fit_transform(X_train)\n",
    "X_train_gram12 = gram12_count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "X_train_VT = VT.fit_transform(X_train_count)\n",
    "\n",
    "# check the content of a document vector\n",
    "print(X_train_gram12.shape)\n",
    "print(X_train_gram12[:10].toarray())\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_train_tfidf[:10].toarray())\n",
    "\n",
    "print(\"check the size of the constructed vocabulary\")\n",
    "print(len(unigram_bool_vectorizer.vocabulary_))\n",
    "print(len(gram12_count_vectorizer.vocabulary_))\n",
    "print(len(unigram_tfidf_vectorizer.vocabulary_))\n",
    "\n",
    "# print out the first 10 items in the vocabulary\n",
    "print(list(unigram_bool_vectorizer.vocabulary_.items())[:10])\n",
    "print(list(gram12_count_vectorizer.vocabulary_.items())[:10])\n",
    "print(list(unigram_tfidf_vectorizer.vocabulary_.items())[:10])\n",
    "\n",
    "# check word index in vocabulary\n",
    "print(unigram_bool_vectorizer.vocabulary_.get('restaurant'))\n",
    "print(gram12_count_vectorizer.vocabulary_.get('restaurant'))\n",
    "print(unigram_tfidf_vectorizer.vocabulary_.get('restaurant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 21)\n",
      "(37, 321)\n",
      "(37, 321)\n",
      "(37, 1)\n"
     ]
    }
   ],
   "source": [
    "# The vectorizer can do \"fit\" and \"transform\"\n",
    "# fit is a process to collect unique tokens into the vocabulary\n",
    "# transform is a process to convert each document to vector based on the vocabulary\n",
    "\n",
    "X_test_tfidf = unigram_tfidf_vectorizer.transform(X_test)\n",
    "X_test_stem = stem_vectorizer.transform(X_test)\n",
    "X_test_bool = unigram_bool_vectorizer.transform(X_test)\n",
    "#sel_X_train_bool = sel.fit_transform(X_test_bool)\n",
    "\n",
    "X_test_count = unigram_count_vectorizer.transform(X_test)\n",
    "X_test_gram12 = gram12_count_vectorizer.transform(X_test)\n",
    "X_test_stem_vec = stem_vectorizer.transform(X_test)\n",
    "\n",
    "X_test_VT = VT.transform(X_test_count)\n",
    "# print out #examples and #features in the test set\n",
    "print(X_test_gram12.shape)\n",
    "print(X_test_tfidf.shape)\n",
    "print(X_test_bool.shape)\n",
    "print(X_test_VT.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Train a MNB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the MNB module\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# initialize the MNB model\n",
    "nb_clf= MultinomialNB()\n",
    "\n",
    "\n",
    "# use the training data to train the MNB model\n",
    "nb_clf.fit(X_train_gram12,y_train)\n",
    "nb_clf.fit(X_train_gram12,z_train)\n",
    "\n",
    "nb_clf.fit(X_train_tfidf,y_train)\n",
    "nb_clf.fit(X_train_tfidf,z_train)\n",
    "\n",
    "nb_clf.fit(X_train_bool,y_train)\n",
    "nb_clf.fit(X_train_bool,z_train)\n",
    "\n",
    "nb_clf.fit(X_train_stem,y_train)\n",
    "nb_clf.fit(X_train_stem,z_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel_X_train_bool = sel.fit_transform(X_train_bool)\n",
    "\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel_X_train_bool = sel.fit_transform(X_train_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4.1 Interpret a trained MNB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.99513791 -4.40060302 -3.48431229 -3.14784005 -4.40060302 -3.99513791\n",
      "  -3.99513791 -4.40060302 -3.99513791 -3.30199073 -4.40060302 -3.99513791\n",
      "  -2.52880084 -3.99513791 -3.30199073 -3.14784005 -3.70745584 -5.0937502\n",
      "  -3.99513791 -3.99513791 -3.70745584 -3.70745584 -3.99513791 -4.40060302\n",
      "  -3.99513791 -3.01430866 -3.99513791 -3.99513791 -3.48431229 -2.09801793\n",
      "  -4.40060302 -4.40060302 -4.40060302 -3.99513791 -4.40060302 -3.99513791\n",
      "  -5.0937502  -5.0937502  -3.99513791 -3.01430866]]\n",
      "[[-3.99513791 -4.40060302 -3.48431229 -3.14784005 -4.40060302 -3.99513791\n",
      "  -3.99513791 -4.40060302 -3.99513791 -3.30199073 -4.40060302 -3.99513791\n",
      "  -2.52880084 -3.99513791 -3.30199073 -3.14784005 -3.70745584 -5.0937502\n",
      "  -3.99513791 -3.99513791 -3.70745584 -3.70745584 -3.99513791 -4.40060302\n",
      "  -3.99513791 -3.01430866 -3.99513791 -3.99513791 -3.48431229 -2.09801793\n",
      "  -4.40060302 -4.40060302 -4.40060302 -3.99513791 -4.40060302 -3.99513791\n",
      "  -5.0937502  -5.0937502  -3.99513791 -3.01430866]]\n"
     ]
    }
   ],
   "source": [
    "## interpreting naive Bayes models\n",
    "## by consulting the sklearn documentation you can also find out how to print the coef_ for naive Bayes \n",
    "## which are the conditional probabilities\n",
    "## http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "\n",
    "# the code below will print out the conditional prob of the word \"worthless\" in each category\n",
    "# sample output\n",
    "# -8.98942647599 -> logP('worthless'|very negative')\n",
    "# -11.1864401922 -> logP('worthless'|negative')\n",
    "# -12.3637684625 -> logP('worthless'|neutral')\n",
    "# -11.9886066961 -> logP('worthless'|positive')\n",
    "# -11.0504454621 -> logP('worthless'|very positive')\n",
    "# the above output means the word feature \"worthless\" is indicating \"very negative\" \n",
    "# because P('worthless'|very negative) is the greatest among all conditional probs\n",
    "\n",
    "stem_vectorizer.vocabulary_.get('owners')\n",
    "for i in range(0,1):\n",
    "  print(nb_clf.coef_[i][stem_vectorizer.vocabulary_.get('owners')])\n",
    "\n",
    "stem_vectorizer.vocabulary_.get('owners')\n",
    "for i in range(0,1):\n",
    "  print(nb_clf.coef_[i][gram12_count_vectorizer.vocabulary_.get('owners')])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-3.147840051751449, 'asked'), (-3.0143086591269266, 'believe'), (-3.0143086591269266, 'bring'), (-2.5288008433452256, 'applied'), (-2.0980179272527715, 'birthday')]\n"
     ]
    }
   ],
   "source": [
    "# sort the conditional probability for category 0 \"very negative\"\n",
    "# print the words with highest conditional probs\n",
    "# these can be words popular in the \"very negative\" category alone, or words popular in all cateogires\n",
    "\n",
    "feature_ranks = sorted(zip(nb_clf.coef_[0], unigram_tfidf_vectorizer.get_feature_names()))\n",
    "very_negative_features = feature_ranks[-5:]\n",
    "print(very_negative_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-3.484312288372662, 'acknowledge'), (-3.484312288372662, 'better'), (-3.3019907315787074, 'appealing'), (-3.3019907315787074, 'ask'), (-3.147840051751449, 'agreed'), (-3.147840051751449, 'asked'), (-3.0143086591269266, 'believe'), (-3.0143086591269266, 'bring'), (-2.5288008433452256, 'applied'), (-2.0980179272527715, 'birthday')]\n",
      "[(-3.484312288372662, 'acknowledge'), (-3.484312288372662, 'better'), (-3.3019907315787074, 'appealing'), (-3.3019907315787074, 'ask'), (-3.147840051751449, 'agreed'), (-3.147840051751449, 'asked'), (-3.0143086591269266, 'believe'), (-3.0143086591269266, 'bring'), (-2.5288008433452256, 'applied'), (-2.0980179272527715, 'birthday')]\n"
     ]
    }
   ],
   "source": [
    "# sort the conditional probability for category 0 \"postive\"\n",
    "# print the words with highest conditional probs\n",
    "# these can be words popular in the \"positive\" category alone, or words popular in all cateogires\n",
    "\n",
    "positive = sorted(zip(nb_clf.coef_[0], unigram_tfidf_vectorizer.get_feature_names()))\n",
    "positive = feature_ranks[-10:]\n",
    "\n",
    "real = sorted(zip(nb_clf.coef_[0], unigram_tfidf_vectorizer.get_feature_names()))\n",
    "real = feature_ranks[-10:]\n",
    "print(positive)\n",
    "print(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty print of top and bottom features\n",
    "\n",
    "# This is a function I found from stackexchange, and adapted a little bit\n",
    "# The purpose is to print the top and bottom features nicely\n",
    "# https://stackoverflow.com/questions/11116697/how-to-get-most-informative-features-for-scikit-learn-classifiers\n",
    "\n",
    "# You can find many useful scripts from stackexchange or GitHub\n",
    "# Most tasks are not so unique, so someone in this world might have done something similar and shared their code\n",
    "\n",
    "def show_most_and_least_informative_features(vectorizer, clf, class_idx=0, n=10):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[class_idx], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[-n:])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive-negative\n",
      "\t-5.0938\tatmosphere     \t\t-3.4843\tacknowledge    \n",
      "\t-5.0938\tboat           \t\t-3.4843\tbetter         \n",
      "\t-5.0938\tbox            \t\t-3.3020\tappealing      \n",
      "\t-4.4006\t6pm            \t\t-3.3020\task            \n",
      "\t-4.4006\tair            \t\t-3.1478\tagreed         \n",
      "\t-4.4006\tambiance       \t\t-3.1478\tasked          \n",
      "\t-4.4006\tappetizer      \t\t-3.0143\tbelieve        \n",
      "\t-4.4006\tbeef           \t\t-3.0143\tbring          \n",
      "\t-4.4006\tbit            \t\t-2.5288\tapplied        \n",
      "\t-4.4006\tbland          \t\t-2.0980\tbirthday       \n",
      "true-false\n",
      "\t-5.0938\tatmosphere     \t\t-3.4843\tacknowledge    \n",
      "\t-5.0938\tboat           \t\t-3.4843\tbetter         \n",
      "\t-5.0938\tbox            \t\t-3.3020\tappealing      \n",
      "\t-4.4006\t6pm            \t\t-3.3020\task            \n",
      "\t-4.4006\tair            \t\t-3.1478\tagreed         \n",
      "\t-4.4006\tambiance       \t\t-3.1478\tasked          \n",
      "\t-4.4006\tappetizer      \t\t-3.0143\tbelieve        \n",
      "\t-4.4006\tbeef           \t\t-3.0143\tbring          \n",
      "\t-4.4006\tbit            \t\t-2.5288\tapplied        \n",
      "\t-4.4006\tbland          \t\t-2.0980\tbirthday       \n",
      "positive-negative\n",
      "\t-5.0938\twent restaurant\t\t-3.9951\twent xyz       \n",
      "\t-4.4006\tbest dining experience\t\t-3.9951\twent xyz restaurant\n",
      "\t-4.4006\texperience life\t\t-3.7075\ttwin trees cicero\n",
      "\t-4.4006\tfood exceptional\t\t-3.7075\txyz restaurant \n",
      "\t-4.4006\tmarshall street\t\t-3.4843\tbest experience\n",
      "\t-3.9951\tbest dining    \t\t-3.3020\tjapanese restaurant\n",
      "\t-3.9951\tfelt like      \t\t-3.3020\ttrees cicero   \n",
      "\t-3.9951\tfinished eating\t\t-3.1478\tdining experience\n",
      "\t-3.9951\tgood place     \t\t-3.1478\ttwin trees     \n",
      "\t-3.9951\tplace called   \t\t-2.5288\tpopular recently\n",
      "true-false\n",
      "\t-5.0938\twent restaurant\t\t-3.9951\twent xyz       \n",
      "\t-4.4006\tbest dining experience\t\t-3.9951\twent xyz restaurant\n",
      "\t-4.4006\texperience life\t\t-3.7075\ttwin trees cicero\n",
      "\t-4.4006\tfood exceptional\t\t-3.7075\txyz restaurant \n",
      "\t-4.4006\tmarshall street\t\t-3.4843\tbest experience\n",
      "\t-3.9951\tbest dining    \t\t-3.3020\tjapanese restaurant\n",
      "\t-3.9951\tfelt like      \t\t-3.3020\ttrees cicero   \n",
      "\t-3.9951\tfinished eating\t\t-3.1478\tdining experience\n",
      "\t-3.9951\tgood place     \t\t-3.1478\ttwin trees     \n",
      "\t-3.9951\tplace called   \t\t-2.5288\tpopular recently\n"
     ]
    }
   ],
   "source": [
    "# show most positive features (category 4)\n",
    "print(\"positive-negative\")\n",
    "show_most_and_least_informative_features(unigram_tfidf_vectorizer, nb_clf, class_idx=0, n=10)\n",
    "print(\"true-false\")\n",
    "show_most_and_least_informative_features(unigram_tfidf_vectorizer, nb_clf, class_idx=0, n=10)\n",
    "\n",
    "# show most positive features (category 4)\n",
    "print(\"positive-negative\")\n",
    "show_most_and_least_informative_features(gram12_count_vectorizer, nb_clf, class_idx=0, n=10)\n",
    "print(\"true-false\")\n",
    "show_most_and_least_informative_features(gram12_count_vectorizer, nb_clf, class_idx=0, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_new \n",
      "   (0, 0)\t1\n",
      "  (10, 0)\t1\n",
      "  (14, 0)\t1\n",
      "  (18, 0)\t1\n",
      "  (22, 0)\t1\n",
      "  (23, 0)\t1\n",
      "  (26, 0)\t1\n",
      "  (33, 0)\t1\n",
      "  (37, 0)\t1\n",
      "  (42, 0)\t2\n",
      "  (52, 0)\t1\n",
      "  (53, 0)\t1\n",
      "  (54, 0)\t1\n",
      "X_newtf \n",
      "   (0, 0)\t0.07755537782497592\n",
      "  (10, 0)\t0.17793928209058169\n",
      "  (14, 0)\t0.14792327783504614\n",
      "  (18, 0)\t0.18684314815978417\n",
      "  (22, 0)\t0.31937437619961495\n",
      "  (23, 0)\t0.11552358499673741\n",
      "  (26, 0)\t0.10165322847399401\n",
      "  (33, 0)\t0.08023216920914446\n",
      "  (37, 0)\t0.2545624471187513\n",
      "  (42, 0)\t0.41534253430612766\n",
      "  (52, 0)\t0.10528246827684114\n",
      "  (53, 0)\t0.18355573600372138\n",
      "  (54, 0)\t0.15588761705592066\n"
     ]
    }
   ],
   "source": [
    "#print(\"y\")\n",
    "#nb_clfy.coef_[0][1]\n",
    "#print(\"z\")\n",
    "#nb_clfz.coef_[0][1]\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "X_new = SelectKBest(chi2, k=1).fit_transform(X_train_stem, y_train)\n",
    "X_new.shape\n",
    "print(\"X_new \\n\",X_new)\n",
    "X_newtf = SelectKBest(chi2, k=1).fit_transform(X_train_tfidf, y_train)\n",
    "X_newtf.shape\n",
    "print(\"X_newtf \\n\", X_newtf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Features:\n",
      "1.4456088276651355 acknowledge\n",
      "1.2511246597660637 asked\n",
      "1.2223352667303482 applied\n",
      "1.2111096288165546 believe\n",
      "1.1716405021888427 bag\n",
      "1.1429336430591281 birthday\n",
      "1.1303063525298733 better\n",
      "1.1055931218644974 ask\n",
      "1.087272959628166 bread\n",
      "1.087272959628166 blast\n",
      "1.087272959628166 bento\n",
      "1.087272959628166 began\n",
      "1.087272959628166 beautiful\n",
      "1.087272959628166 authentic\n",
      "1.087272959628166 area\n",
      "1.087272959628166 applebee\n",
      "1.087272959628166 alfredo\n",
      "1.0254565152108925 bring\n",
      "0.9857833197145557 blue\n",
      "0.9857833197145557 bad\n",
      "0.9857833197145557 american\n",
      "0.9857833197145557 5pm\n",
      "0.9846801685983319 ate\n",
      "0.936114818190439 appealing\n",
      "0.924492384586033 bar\n",
      "0.902119541778304 agreed\n",
      "0.8949546904425452 blend\n",
      "0.8949546904425452 blanking\n",
      "0.8949546904425452 beef\n",
      "0.8949546904425452 appetizer\n",
      "Bottom Features:\n",
      "0.7166936142089452 atmosphere\n",
      "0.7166936142089452 boat\n",
      "0.7166936142089452 box\n",
      "0.7737010639774482 best\n",
      "0.8295813606674715 6pm\n",
      "0.8295813606674715 bit\n",
      "0.8295813606674715 bland\n",
      "0.857921494916488 amazing\n",
      "0.8949546904425452 air\n",
      "0.8949546904425452 ambiance\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate log ratio of conditional probs\n",
    "\n",
    "# In this exercise you will calculate the log ratio \n",
    "# between conditional probs in the \"very negative\" category\n",
    "# and conditional probs in the \"very positive\" category,\n",
    "# and then sort and print out the top and bottom 10 words\n",
    "\n",
    "# the conditional probs for the \"very negative\" category is stored in nb_clf.coef_[0]\n",
    "# the conditional probs for the \"very positive\" category is stored in nb_clf.coef_[4]\n",
    "\n",
    "# You can consult with similar code in week 4's sample script on feature weighting\n",
    "# Note that in sklearn's MultinomialNB the conditional probs have been converted to log values.\n",
    "\n",
    "# Your code starts here\n",
    "\n",
    "ratios = (nb_clf.feature_log_prob_[0]/nb_clf.feature_log_prob_[1])\n",
    "feature_ranks = sorted(zip(ratios,unigram_count_vectorizer.get_feature_names()))\n",
    "bottom_features = feature_ranks[:10]\n",
    "top_features = feature_ranks[10:]\n",
    "print(\"Top Features:\")\n",
    "[print(x[0],x[1]) for x in reversed(top_features) ]\n",
    "print(\"Bottom Features:\")\n",
    "[print(x[0],x[1]) for x in bottom_features]\n",
    "\n",
    "\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Test the MNB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 321)\n",
      "(37,)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(37, 21)\n",
      "(37,)\n",
      "(55, 21)\n",
      "(55,)\n",
      "(55,)\n",
      "  (4, 3)\t1\n",
      "  (4, 4)\t1\n",
      "  (10, 11)\t1\n"
     ]
    }
   ],
   "source": [
    "# test the classifier on the test data set, print accuracy score\n",
    "print(X_test_bool.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "#print(X_train_gram12[:10].toarray())\n",
    "print(X_test_bool[:10].toarray())\n",
    "\n",
    "#nb_clf.score(X_test_bool,y_test)\n",
    "\n",
    "\n",
    "#nb_clfy.score(X_test_vec,y_test)\n",
    "\n",
    "print(X_test_gram12.shape)\n",
    "print(y_test.shape)\n",
    "print(X_train_gram12.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(z_train.shape)\n",
    "\n",
    "print(X_test_gram12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test tfidf precision and recall\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'precision_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a525121705b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test tfidf precision and recall\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train tfidf CM for positive and negative review\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'precision_score' is not defined"
     ]
    }
   ],
   "source": [
    "# print confusion matrix (row: ground truth; col: prediction)\n",
    "#X_train_tfidf = unigram_tfidf_vectorizer.fit_transform(X_train)\n",
    "#X_train_bool = unigram_bool_vectorizer.fit_transform(X_train)\n",
    "#X_train_count = unigram_count_vectorizer.fit_transform(X_train)\n",
    "#X_train_gram12 = gram12_count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#y_true = y_test[1]\n",
    "y_pred = nb_clf.fit(X_train_tfidf, y_train).predict(X_test_tfidf)\n",
    "z_pred = nb_clf.fit(X_train_tfidf, z_train).predict(X_test_tfidf)\n",
    "cmy=confusion_matrix(y_test, y_pred)\n",
    "cmz=confusion_matrix(z_test, z_pred)\n",
    "\n",
    "print(\"Test tfidf precision and recall\" )\n",
    "print(precision_score(y_test, y_pred, average=None))\n",
    "print(recall_score(y_test, y_pred, average=None))\n",
    "print(\"Train tfidf CM for positive and negative review\")\n",
    "print(cmy)\n",
    "\n",
    "print(\"Train tfidf CM for true and false review\")\n",
    "print(cmz)\n",
    "\n",
    "y_predbool = nb_clf.fit(X_train_bool, y_train).predict(X_test_bool)\n",
    "z_predbool = nb_clf.fit(X_train_bool, z_train).predict(X_test_bool)\n",
    "cmy=confusion_matrix(y_test, y_predbool)\n",
    "cmz=confusion_matrix(z_test, z_predbool)\n",
    "\n",
    "print(\"Test boolean prcision and recall\")\n",
    "print(precision_score(y_test, y_predbool, average=None))\n",
    "print(recall_score(y_test, y_predbool, average=None))\n",
    "print(cmy)\n",
    "\n",
    "print(\"Test tfidf CM for true and false review\")\n",
    "print(cmz)\n",
    "\n",
    "y_count = nb_clf.fit(X_train_count, y_train).predict(X_test_count)\n",
    "z_count = nb_clf.fit(X_train_count, z_train).predict(X_test_count)\n",
    "cmycount=confusion_matrix(y_test, y_count)\n",
    "cmzcount=confusion_matrix(z_test, y_count)\n",
    "\n",
    "print(precision_score(y_test, y_count, average=None))\n",
    "print(recall_score(y_test, y_count, average=None))\n",
    "\n",
    "print(\"Test boolean CM for positive and negative review\")\n",
    "print(cmycount)\n",
    "\n",
    "print(\"Test boolean CM for true and false review\")\n",
    "print(cmzcount)\n",
    "\n",
    "y_pstem = nb_clf.fit(X_train_stem, y_train).predict(X_test_stem)\n",
    "z_pstem = nb_clf.fit(X_train_stem, z_train).predict(X_test_stem)\n",
    "cmystem=confusion_matrix(y_test, y_pstem)\n",
    "cmzstem=confusion_matrix(z_test, z_pstem)\n",
    "\n",
    "print(precision_score(y_test, y_pstem, average=None))\n",
    "print(recall_score(y_test, y_pstem, average=None))\n",
    "\n",
    "print(\"Test stem CM for positive and negative review\")\n",
    "print(cmystem)\n",
    "\n",
    "print(\"Test stem CM for true and false review\")\n",
    "print(cmzstem)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = nb_clf.fit(X_train_bool, y_train).predict(X_test_bool)\n",
    "z_pred = nb_clf.fit(X_train_bool, z_train).predict(X_test_bool)\n",
    "\n",
    "cmyg12=confusion_matrix(y_test, y_pred)\n",
    "cmzg12=confusion_matrix(z_test, z_pred)\n",
    "\n",
    "print(precision_score(y_test, y_predg12, average=None))\n",
    "print(recall_score(y_test, y_predg12, average=None))\n",
    "\n",
    "print(\"Test bool NB CM for positive and negative review\")\n",
    "print(cmyg12)\n",
    "\n",
    "print(\"Test bool NB CM for true and false review\")\n",
    "print(cmzg12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(X_train_bool.shape)\n",
    "\n",
    "#y_true = y_test[1]\n",
    "y_pred = nb_clf.fit(X_train_bool, y_train).predict(X_test_bool)\n",
    "z_pred = nb_clf.fit(X_train_bool, z_train).predict(X_test_bool)\n",
    "cmy=confusion_matrix(y_test, y_pred)\n",
    "cmz=confusion_matrix(z_test, z_pred)\n",
    "print(\"Test bool precision and recall y\" )\n",
    "target_names = ['0','1','2','3']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "print(precision_score(y_test, y_pred, average=None))\n",
    "print(recall_score(y_test, y_pred, average=None))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(cmy)\n",
    "\n",
    "print(\"\\n \\n\")\n",
    "print(\"Test bool precision and recall accuracy z\" )\n",
    "print(classification_report(z_test, z_pred, target_names=target_names))\n",
    "print(precision_score(z_test, z_pred, average=None))\n",
    "print(recall_score(z_test, z_pred, average=None))\n",
    "print(accuracy_score(z_test, z_pred))\n",
    "\n",
    "print(cmz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(X_train_tfidf.shape)\n",
    "\n",
    "#y_true = y_test[1]\n",
    "y_pred = nb_clf.fit(X_train_tfidf, y_train).predict(X_test_tfidf)\n",
    "z_pred = nb_clf.fit(X_train_tfidf, z_train).predict(X_test_tfidf)\n",
    "cmy=confusion_matrix(y_test, y_pred)\n",
    "cmz=confusion_matrix(z_test, z_pred)\n",
    "print(\"Test tfidf precision and recall y\" )\n",
    "target_names = ['0','1','2','3']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "print(precision_score(y_test, y_pred, average=None))\n",
    "print(recall_score(y_test, y_pred, average=None))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(cmy)\n",
    "\n",
    "print(\"\\n \\n\")\n",
    "print(\"Test tfidf precision and recall accuracy z\" )\n",
    "print(classification_report(z_test, z_pred, target_names=target_names))\n",
    "print(precision_score(z_test, z_pred, average=None))\n",
    "print(recall_score(z_test, z_pred, average=None))\n",
    "print(accuracy_score(z_test, z_pred))\n",
    "\n",
    "print(cmz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(X_train_stem.shape)\n",
    "\n",
    "#y_true = y_test[1]\n",
    "y_pred = nb_clf.fit(X_train_stem, y_train).predict(X_test_stem)\n",
    "z_pred = nb_clf.fit(X_train_stem, z_train).predict(X_test_stem)\n",
    "cmy=confusion_matrix(y_test, y_pred)\n",
    "cmz=confusion_matrix(z_test, z_pred)\n",
    "print(\"Test stem precision and recall y\" )\n",
    "target_names = ['0','1','2','3']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "print(precision_score(y_test, y_pred, average=None))\n",
    "print(recall_score(y_test, y_pred, average=None))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(cmy)\n",
    "\n",
    "print(\"\\n \\n\")\n",
    "print(\"Test stem precision and recall accuracy z\" )\n",
    "print(classification_report(z_test, z_pred, target_names=target_names))\n",
    "print(precision_score(z_test, z_pred, average=None))\n",
    "print(recall_score(z_test, z_pred, average=None))\n",
    "print(accuracy_score(z_test, z_pred))\n",
    "\n",
    "print(cmz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BenoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "BNB = BernoulliNB()\n",
    "print(X_train_bool.shape)\n",
    "\n",
    "y_predb = BNB.fit(X_train_bool, y_train).predict(X_test_bool)\n",
    "z_predb = BNB.fit(X_train_bool, z_train).predict(X_test_bool)\n",
    "\n",
    "cmyb=confusion_matrix(y_test, y_predb)\n",
    "cmzb=confusion_matrix(z_test, z_predb)\n",
    "\n",
    "print(\"Test BNB precision and recall y\" )\n",
    "target_names = ['0','1','2','3']\n",
    "print(classification_report(y_test, y_predb, target_names=target_names))\n",
    "\n",
    "print(precision_score(y_test, y_predb, average=None))\n",
    "print(recall_score(y_test, y_predb, average=None))\n",
    "print(accuracy_score(y_test, y_predb))\n",
    "print(cmyb)\n",
    "\n",
    "print(\"\\n \\n\")\n",
    "print(\"Test BNB precision and recall accuracy z\" )\n",
    "print(classification_report(z_test, z_predb, target_names=target_names))\n",
    "print(precision_score(z_test, z_predb, average=None))\n",
    "print(recall_score(z_test, z_predb, average=None))\n",
    "print(accuracy_score(z_test, z_predb))\n",
    "print(cmzb)\n",
    "\n",
    "print(\"\\n \\n\")\n",
    "\n",
    "\n",
    "print(X_train_stem.shape)\n",
    "\n",
    "y_pred = BNB.fit(X_train_stem, y_train).predict(X_test_stem)\n",
    "z_pred = BNB.fit(X_train_stem, z_train).predict(X_test_stem)\n",
    "\n",
    "print(precision_score(y_test, y_predg12, average=None))\n",
    "print(recall_score(y_test, y_predg12, average=None))\n",
    "\n",
    "cmy=confusion_matrix(y_test, y_pred)\n",
    "cmz=confusion_matrix(z_test, z_pred)\n",
    "\n",
    "print(\"Test BNB/stem precision and recall y\" )\n",
    "target_names = ['0','1','2','3']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "print(precision_score(y_test, y_pred, average=None))\n",
    "print(recall_score(y_test, y_pred, average=None))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(cmy)\n",
    "\n",
    "print(\"\\n \\n\")\n",
    "print(\"Test BNB/stem precision and recall accuracy z\" )\n",
    "print(classification_report(z_test, z_pred, target_names=target_names))\n",
    "print(precision_score(z_test, z_pred, average=None))\n",
    "print(recall_score(z_test, z_pred, average=None))\n",
    "print(accuracy_score(z_test, z_pred))\n",
    "\n",
    "print(cmz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_tfidf.shape)\n",
    "\n",
    "#y_true = y_test[1]\n",
    "y_pred = nb_clf.fit(X_train_tfidf, y_train).predict(X_test_tfidf)\n",
    "z_pred = nb_clf.fit(X_train_tfidf, z_train).predict(X_test_tfidf)\n",
    "cmy=confusion_matrix(y_test, y_pred)\n",
    "cmz=confusion_matrix(z_test, z_pred)\n",
    "print(\"Test tfidf precision and recall y\" )\n",
    "target_names = ['0','1','2','3']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "print(precision_score(y_test, y_pred, average=None))\n",
    "print(recall_score(y_test, y_pred, average=None))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(cmy)\n",
    "\n",
    "print(\"\\n \\n\")\n",
    "print(\"Test tfidf precision and recall accuracy z\" )\n",
    "print(classification_report(y_test, z_pred, target_names=target_names))\n",
    "print(precision_score(z_test, z_pred, average=None))\n",
    "print(recall_score(z_test, z_pred, average=None))\n",
    "print(accuracy_score(z_test, z_pred))\n",
    "\n",
    "print(cmz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=False)),('nb', MultinomialNB())])\n",
    "scoresy = cross_val_score(nb_clf_pipe, X, y, cv=10)\n",
    "avgy=sum(scoresy)/len(scoresy)\n",
    "print(avgy)\n",
    "print(scoresy[:10])\n",
    "\n",
    "\n",
    "scoresz = cross_val_score(nb_clf_pipe, X, z, cv=10)\n",
    "avgz=sum(scoresz)/len(scoresz)\n",
    "print(avgz)\n",
    "print(scoresz[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 3-fold cross validation to compare the performance of \n",
    "# (1) BernoulliNB (2) MultinomialNB with TF vectors (3) MultinomialNB with boolean vectors\n",
    "\n",
    "# Your code starts here\n",
    "# cross validation\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "nb_clf_pipe = Pipeline([('vect', CountVectorizer(encoding='latin-1', binary=False)),('nb', BernoulliNB())])\n",
    "\n",
    "scores = cross_val_score(nb_clf_pipe, X, y, cv=10)\n",
    "avg=sum(scores)/len(scores)\n",
    "print(avg)\n",
    "print(scores[:10])\n",
    "\n",
    "\n",
    "scoresz = cross_val_score(nb_clf_pipe, X, z, cv=10)\n",
    "avgz=sum(scoresz)/len(scoresz)\n",
    "print(avgz)\n",
    "print(scoresz[:10])\n",
    "\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5.1 Interpret the prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find the calculated posterior probability\n",
    "posterior_probs = nb_clf.predict_proba(X_test_tfidf)\n",
    "\n",
    "## find the posterior probabilities for the first test example\n",
    "print(posterior_probs[0])\n",
    "\n",
    "# find the category prediction for the first test example\n",
    "y_pred = nb_clf.predict(X_test_tfidf)\n",
    "print(y_pred[0])\n",
    "\n",
    "# check the actual label for the first test example\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5.2 Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out specific type of error for further analysis\n",
    "\n",
    "# print out the very positive examples that are mistakenly predicted as negative\n",
    "# according to the confusion matrix, there should be 53 such examples\n",
    "# note if you use a different vectorizer option, your result might be different\n",
    "\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test)):\n",
    "    if(y_test[i]==1 and y_predtfidf[i]==1):\n",
    "        print(X_test_tfidf[i])\n",
    "        err_cnt = err_cnt+1\n",
    "print(\"errors:\", err_cnt)\n",
    "\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test)):\n",
    "    if(y_test[i]==1 and y_predg12[i]==1):\n",
    "        print(X_test_bool[i])\n",
    "        err_cnt = err_cnt+1\n",
    "print(\"errors:\", err_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you find linguistic patterns in the above errors? \n",
    "# What kind of very positive examples were mistakenly predicted as negative?\n",
    "\n",
    "# Can you write code to print out the errors that very negative examples were mistakenly predicted as very positive?\n",
    "# Can you find lingustic patterns for this kind of errors?\n",
    "# Based on the above error analysis, what suggestions would you give to improve the current model?\n",
    "\n",
    "# Your code starts here\n",
    "err_cnt = 0\n",
    "for i in range(0, len(y_test)):\n",
    "    if(y_test[i]==4 and y_pred[i]==0):\n",
    "        print(X_test_tfidf[i])\n",
    "        err_cnt = err_cnt+1\n",
    "print(\"errors:\", err_cnt)\n",
    "# Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: write the prediction output to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=nb_clf.predict(X_test_tfidf)\n",
    "print(y_pred)\n",
    "output = open('/Users/kenmckee/Desktop/GS/S18/tm/HW6/prediction_outputHW6.csv', 'w')\n",
    "for x, value in enumerate(y_pred):\n",
    "  output.write(str(value) + '\\n') \n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6.1 Prepare submission to Kaggle sentiment classification competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('/Users/kenmckee/Desktop/GS/S18/tm/HW6/X.tsv', 'w')\n",
    "for x, value in enumerate(X_train):\n",
    "  output.write(str(value) + '\\n') \n",
    "output.close()\n",
    "\n",
    "output = open('/Users/kenmckee/Desktop/GS/S18/tm/HW6/testY.tsv', 'w')\n",
    "for x, value in enumerate(y_train):\n",
    "  output.write(str(value) + '\\n') \n",
    "output.close()\n",
    " \n",
    "output.close()\n",
    "output = open('/Users/kenmckee/Desktop/GS/S18/tm/HW6/testZ.tsv', 'w')\n",
    "for x, value in enumerate(z_train):\n",
    "  output.write(str(value) + '\\n') \n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## submit to HW6 submission\n",
    "\n",
    "# we are still using the model trained on 60% of the training data\n",
    "# you can re-train the model on the entire data set \n",
    "#   and use the new model to predict the HW6 test data\n",
    "# below is sample code for using a trained model to predict HW6 test data \n",
    "#    and format the prediction output for HW6 submission\n",
    "\n",
    "# read in the test data\n",
    "HW6_testX=p.read_csv(\"/Users/kenmckee/Desktop/GS/S18/tm/HW6//testX.tsv\", delimiter='\\t') \n",
    "HW6_idsY=p.read_csv(\"/Users/kenmckee/Desktop/GS/S18/tm/HW6//testY.tsv\", delimiter='\\t') \n",
    "HW6_testZ=p.read_csv(\"/Users/kenmckee/Desktop/GS/S18/tm/HW6//testZ.tsv\", delimiter='\\t') \n",
    "\n",
    "#print(HW6_idsY)\n",
    "\n",
    "# vectorize the test examples using the vocabulary fitted from the 60% training data\n",
    "HW6_X_test_vec=unigram_tfidf_vectorizer.transform(HW6_testX)\n",
    "\n",
    "# predict using the NB classifier that we built\n",
    "HW6_pred=nb_clf.fit(X_train_tfidf, y_train).predict(HW6_X_test_vec)\n",
    "\n",
    "# combine the test example ids with their predictions\n",
    "HW6_submission=zip(HW6_idsY, HW6_pred)\n",
    "\n",
    "print(HW6_X_test_vec)\n",
    "\n",
    "# prepare output file\n",
    "outf=open('/Users/kenmckee/Desktop/GS/S18/tm/HW6/HW6_submission.csv', 'w')\n",
    "\n",
    "# write header\n",
    "outf.write('PhraseId,Sentiment\\n')\n",
    "\n",
    "# write predictions with ids to the output file\n",
    "for x, value in enumerate(HW6_submission): outf.write(str(value[0]) + ',' + str(value[1]) + '\\n')\n",
    "\n",
    "# close the output file\n",
    "outf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate your HW6 submissions with boolean representation and TF representation\n",
    "# read in the test data\n",
    "HW6_test=p.read_csv(\"/Users/kenmckee/Desktop/GS/S18/tm/HW6/test.tsv\", delimiter='\\t') \n",
    "\n",
    "# preserve the id column of the test examples\n",
    "HW6_ids=HW6_test['PhraseId'].values\n",
    "\n",
    "# read in the text content of the examples\n",
    "HW6_X_test=HW6_test['Phrase'].values\n",
    "\n",
    "# vectorize the test examples using the vocabulary fitted from the 60% training data\n",
    "HW6_X_test_vec=unigram_tfidf_vectorizer.transform(HW6_X_test)\n",
    "\n",
    "# predict using the NB classifier that we built\n",
    "HW6_pred=nb_clf.fit(X_train_vec, y_train).predict(HW6_X_test_vec)\n",
    "\n",
    "# combine the test example ids with their predictions\n",
    "HW6_submission=zip(HW6_ids, HW6_pred)\n",
    "\n",
    "# prepare output file\n",
    "outf=open('/Users/kenmckee/Desktop/GS/S18/tm/HW6/HW6_submission.csv', 'w')\n",
    "\n",
    "# write header\n",
    "outf.write('PhraseId,Sentiment\\n')\n",
    "\n",
    "# write predictions with ids to the output file\n",
    "for x, value in enumerate(HW6_submission): outf.write(str(value[0]) + ',' + str(value[1]) + '\\n')\n",
    "\n",
    "# close the output file\n",
    "outf.close()\n",
    "# submit to HW6\n",
    "\n",
    "# report your scores here\n",
    "# which model gave better performance in the hold-out test\n",
    "# which model gave better performance in the HW6 test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
