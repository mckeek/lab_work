{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/t10k-labels-idx1-ubyte.gz\n",
      "Extracting input/data/train-images-idx3-ubyte.gz\n",
      "Extracting input/data/train-labels-idx1-ubyte.gz\n",
      "Extracting input/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting input/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Import Fashion MNIST\n",
    "fashion_mnist = fashion_mnist = input_data.read_data_sets('data/fashion', one_hot=True)\n",
    "fashion_mnistx = fashion_mnist = input_data.read_data_sets('input/data', one_hot=True)\n",
    "\n",
    "#data = input_data.read_data_sets('data/fashion', source_url='http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (images) shape: (55000, 784)\n",
      "Training set (labels) shape: (55000, 10)\n",
      "Test set (images) shape: (10000, 784)\n",
      "Test set (labels) shape: (10000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOutput:\\nTraining set (images) shape: (55000, 784)\\nTraining set (labels) shape: (55000, 10)\\nTest set (images) shape: (10000, 784)\\nTest set (labels) shape: (10000, 10)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shapes of training set\n",
    "print(\"Training set (images) shape: {shape}\".format(shape=fashion_mnist.train.images.shape))\n",
    "print(\"Training set (labels) shape: {shape}\".format(shape=fashion_mnist.train.labels.shape))\n",
    "\n",
    "# Shapes of test set\n",
    "print(\"Test set (images) shape: {shape}\".format(shape=fashion_mnist.test.images.shape))\n",
    "print(\"Test set (labels) shape: {shape}\".format(shape=fashion_mnist.test.labels.shape))\n",
    "\n",
    "'''\n",
    "Output:\n",
    "Training set (images) shape: (55000, 784)\n",
    "Training set (labels) shape: (55000, 10)\n",
    "Test set (images) shape: (10000, 784)\n",
    "Test set (labels) shape: (10000, 10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z),axis=1))\n",
    "    return sm\n",
    "\n",
    "\n",
    "def layers(X, Y):\n",
    "    \"\"\"\n",
    "    :param X:\n",
    "    :param Y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0]\n",
    "    n_y = Y.shape[0]\n",
    "    return n_x, n_y\n",
    "\n",
    "\n",
    "def initialize_nn(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    :param n_x:\n",
    "    :param n_h:\n",
    "    :param n_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    np.random.seed(2)\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.random.rand(n_h, 1)\n",
    "    W2 = np.random.rand(n_y, n_h)\n",
    "    b2 = np.random.rand(n_y, 1)\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def forward_prop(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = softmax(Z2.T)\n",
    "\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "\n",
    "    return A2, cache\n",
    "\n",
    "\n",
    "def compute_cost(A2, Y, parameters):\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    logprobs = np.multiply(np.log(A2), Y)\n",
    "    cost = - np.sum(logprobs) / m\n",
    "    cost = np.squeeze(cost)\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def back_prop(parameters, cache, X, Y):\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "\n",
    "\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.square(A1))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "def update_params(parameters, grads, alpha):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def model_nn(label_dict,X, Y,Y_real,test_x,test_y, n_h, num_iters, alpha, print_cost):\n",
    "    np.random.seed(3)\n",
    "    n_x,n_y = layers(X, Y)\n",
    "    parameters = initialize_nn(n_x, n_h, n_y)\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    costs = []\n",
    "    for i in range(0, num_iters):\n",
    "\n",
    "        A2, cache = forward_prop(X, parameters)\n",
    "\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    "        grads = back_prop(parameters, cache, X, Y)\n",
    "        if (i > 1500):\n",
    "            alpha1 = 0.95*alpha\n",
    "            parameters = update_params(parameters, grads, alpha1)\n",
    "        else:\n",
    "            parameters = update_params(parameters, grads, alpha)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration for %i: %f\" % (i, cost))\n",
    "\n",
    "\n",
    "\n",
    "    predictions = predict_nn(parameters, X)\n",
    "    print(\"Train accuracy: {} %\", sum(predictions == Y_real) / (float(len(Y_real))) * 100)\n",
    "    predictions=predict_nn(parameters,test_x)\n",
    "    print(\"Train accuracy: {} %\", sum(predictions == test_y) / (float(len(test_y))) * 100)\n",
    "\n",
    "\n",
    "\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(alpha))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def predict_nn(parameters, X):\n",
    "    A2, cache = forward_prop(X, parameters)\n",
    "    predictions = np.argmax(A2, axis=0)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z), axis=1))\n",
    "    return sm\n",
    "\n",
    "\n",
    "def initialize(dim1, dim2):\n",
    "    \"\"\"\n",
    "    :param dim: size of vector w initilazied with zeros\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    w = np.zeros(shape=(dim1, dim2))\n",
    "    b = np.zeros(shape=(10, 1))\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    :param w: weights for w\n",
    "    :param b: bias\n",
    "    :param X: size of data(no of features, no of examples)\n",
    "    :param Y: true label\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    m = X.shape[1]  # getting no of rows\n",
    "\n",
    "    # Forward Prop\n",
    "    A = softmax((np.dot(w.T, X) + b).T)\n",
    "    cost = (-1 / m) * np.sum(Y * np.log(A))\n",
    "\n",
    "    # backwar prop\n",
    "    dw = (1 / m) * np.dot(X, (A - Y).T)\n",
    "    db = (1 / m) * np.sum(A - Y)\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    return grads, cost\n",
    "\n",
    "\n",
    "def optimize(w, b, X, Y, num_iters, alpha, print_cost=False):\n",
    "    \"\"\"\n",
    "    :param w: weights for w\n",
    "    :param b: bias\n",
    "    :param X: size of data(no of features, no of examples)\n",
    "    :param Y: true label\n",
    "    :param num_iters: number of iterations for gradient\n",
    "    :param alpha:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    costs = []\n",
    "    for i in range(num_iters):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        w = w - alpha * dw\n",
    "        b = b - alpha * db\n",
    "        alpha = alpha * 0.99\n",
    "\n",
    "        # Record the costs\n",
    "        if i % 50 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 50 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "\n",
    "    return params, grads, costs\n",
    "\n",
    "\n",
    "def predict(w, b, X):\n",
    "    \"\"\"\n",
    "    :param w:\n",
    "    :param b:\n",
    "    :param X:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    y_pred = np.argmax(softmax((np.dot(w.T, X) + b).T), axis=0)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def model_LR(label_dict, X_train, Y_train, Y, test_x, test_y, num_iters, alpha, print_cost):\n",
    "    \"\"\"\n",
    "    :param X_train:\n",
    "    :param Y_train:\n",
    "    :param X_test:\n",
    "    :param Y_test:\n",
    "    :param num_iterations:\n",
    "    :param learning_rate:\n",
    "    :param print_cost:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    w, b = initialize(X_train.shape[0], Y_train.shape[0])\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iters, alpha, print_cost)\n",
    "\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "\n",
    "    y_prediction_train = predict(w, b, X_train)\n",
    "    y_prediction_test = predict(w, b, test_x)\n",
    "    print(\"Train accuracy: {} %\", sum(y_prediction_train == Y) / (float(len(Y))) * 100)\n",
    "    print(\"Test accuracy: {} %\", sum(y_prediction_test == test_y) / (float(len(test_y))) * 100)\n",
    "\n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": y_prediction_test,\n",
    "         \"Y_prediction_train\": y_prediction_train,\n",
    "         \"w\": w,\n",
    "         \"b\": b,\n",
    "         \"learning_rate\": alpha,\n",
    "         \"num_iterations\": num_iters}\n",
    "\n",
    "    # Plot learning curve (with costs)\n",
    "    costs = np.squeeze(d['costs'])\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "    plt.plot()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    pri(X_train, y_prediction_train, label_dict)\n",
    "    return d\n",
    "\n",
    "\n",
    "def pri(X, Y, label):\n",
    "    example = X[:, 2]\n",
    "    print(\"Prediction for the example is \", label[Y[2]])\n",
    "    plt.imshow(np.reshape(example, [28, 28]), cmap='Greys')\n",
    "    plt.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 0 (T-Shirt)\n",
      "y = 0 (T-Shirt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c3384a470>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c2725b198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create dictionary of target classes\n",
    "label_dict = {\n",
    " 0: 'T-Shirt', \n",
    " 1: 'Trouser',\n",
    " 2: 'Pullover',\n",
    " 3: 'Dress',\n",
    " 4: 'Coat',\n",
    " 5: 'Sandal',\n",
    " 6: 'Shirt',\n",
    " 7: 'Sneaker',\n",
    " 8: 'Bag',\n",
    " 9: 'Ankle Boot'\n",
    "}\n",
    "\n",
    "# Get 28x28 image\n",
    "sample_1 = fashion_mnist.train.images[15].reshape(28,28)\n",
    "# Get corresponding integer label from one-hot encoded data\n",
    "sample_label_1 = np.where(fashion_mnist.train.labels[15] == 1)[0][0]\n",
    "# Plot sample\n",
    "print(\"y = {label_index} ({label})\".format(label_index=sample_label_1, label=label_dict[sample_label_1]))\n",
    "plt.imshow(sample_1, cmap='Greys')\n",
    "\n",
    "# Sample 2\n",
    "\n",
    "# Get 28x28 image\n",
    "sample_2 = fashion_mnist.train.images[15].reshape(28,28)\n",
    "# Get corresponding integer label from one-hot encoded data\n",
    "sample_label_2 = np.where(fashion_mnist.train.labels[15] == 1)[0][0]\n",
    "# Plot sample\n",
    "print(\"y = {label_index} ({label})\".format(label_index=sample_label_2, label=label_dict[sample_label_2]))\n",
    "plt.imshow(sample_2, cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 0 (T-Shirt)\n",
      "y = 0 (T-Shirt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c36172b70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c2a8c27f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample 1\n",
    "\n",
    "# Get 28x28 image\n",
    "sample_1 = fashion_mnist.train.images[15].reshape(28,28)\n",
    "# Get corresponding integer label from one-hot encoded data\n",
    "sample_label_1 = np.where(fashion_mnist.train.labels[15] == 1)[0][0]\n",
    "# Plot sample\n",
    "print(\"y = {label_index} ({label})\".format(label_index=sample_label_1, label=label_dict[sample_label_1]))\n",
    "plt.imshow(sample_1, cmap='Greys')\n",
    "\n",
    "# Sample 2\n",
    "\n",
    "# Get 28x28 image\n",
    "sample_2 = fashion_mnist.train.images[15].reshape(28,28)\n",
    "# Get corresponding integer label from one-hot encoded data\n",
    "sample_label_2 = np.where(fashion_mnist.train.labels[15] == 1)[0][0]\n",
    "# Plot sample\n",
    "print(\"y = {label_index} ({label})\".format(label_index=sample_label_2, label=label_dict[sample_label_2]))\n",
    "plt.imshow(sample_2, cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "n_hidden_1 = 128 # Units in first hidden layer\n",
    "n_hidden_2 = 128 # Units in second hidden layer\n",
    "n_input = 784 # Fashion MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # Fashion MNIST total classes (0â€“9 digits)\n",
    "n_samples = fashion_mnist.train.num_examples # Number of examples in training set \n",
    "\n",
    "\n",
    "# Create placeholders\n",
    "def create_placeholders(n_x, n_y):\n",
    " '''\n",
    " Creates the placeholders for the tensorflow session.\n",
    " \n",
    " Arguments:\n",
    " n_x -- scalar, size of an image vector (28*28 = 784)\n",
    " n_y -- scalar, number of classes (10)\n",
    " \n",
    " Returns:\n",
    " X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    " Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    " '''\n",
    " \n",
    " X = tf.placeholder(tf.float32, [n_x, None], name=\"X\")\n",
    " Y = tf.placeholder(tf.float32, [n_y, None], name=\"Y\")\n",
    " \n",
    " return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    '''\n",
    "    Initializes parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [n_hidden_1, n_input]\n",
    "                        b1 : [n_hidden_1, 1]\n",
    "                        W2 : [n_hidden_2, n_hidden_1]\n",
    "                        b2 : [n_hidden_2, 1]\n",
    "                        W3 : [n_classes, n_hidden_2]\n",
    "                        b3 : [n_classes, 1]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    '''\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    tf.set_random_seed(42)\n",
    "    \n",
    "    # Initialize weights and biases for each layer\n",
    "    # First hidden layer\n",
    "    W1 = tf.get_variable(\"W1\", [n_hidden_1, n_input], initializer=tf.contrib.layers.xavier_initializer(seed=42))\n",
    "    b1 = tf.get_variable(\"b1\", [n_hidden_1, 1], initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # Second hidden layer\n",
    "    W2 = tf.get_variable(\"W2\", [n_hidden_2, n_hidden_1], initializer=tf.contrib.layers.xavier_initializer(seed=42))\n",
    "    b2 = tf.get_variable(\"b2\", [n_hidden_2, 1], initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # Output layer\n",
    "    W3 = tf.get_variable(\"W3\", [n_classes, n_hidden_2], initializer=tf.contrib.layers.xavier_initializer(seed=42))\n",
    "    b3 = tf.get_variable(\"b3\", [n_classes, 1], initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # Store initializations as a dictionary of parameters\n",
    "    parameters = {\n",
    "        \"W1\": W1,\n",
    "        \"b1\": b1,\n",
    "        \"W2\": W2,\n",
    "        \"b2\": b2,\n",
    "        \"W3\": W3,\n",
    "        \"b3\": b3\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    '''\n",
    "    Implements the forward propagation for the model: \n",
    "    LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    '''\n",
    "    \n",
    "    # Retrieve parameters from dictionary\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    # Carry out forward propagation      \n",
    "    Z1 = tf.add(tf.matmul(W1,X), b1)     \n",
    "    A1 = tf.nn.relu(Z1)                  \n",
    "    Z2 = tf.add(tf.matmul(W2,A1), b2)    \n",
    "    A2 = tf.nn.relu(Z2)                  \n",
    "    Z3 = tf.add(tf.matmul(W3,A2), b3)    \n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    '''\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (10, number_of_examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    '''\n",
    "    \n",
    "    # Get logits (predictions) and labels\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    # Compute cost\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train, test, learning_rate=0.0001, num_epochs=16, minibatch_size=32, print_cost=True, graph_filename='costs'):\n",
    "    '''\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    train -- training set\n",
    "    test -- test set\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every epoch\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    '''\n",
    "    \n",
    "    # Ensure that model can be rerun without overwriting tf variables\n",
    "    ops.reset_default_graph()\n",
    "    # For reproducibility\n",
    "    tf.set_random_seed(42)\n",
    "    seed = 42\n",
    "    # Get input and output shapes\n",
    "    (n_x, m) = train.images.T.shape\n",
    "    n_y = train.labels.T.shape[0]\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    # Create placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    # Forward propagation\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    # Cost function\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    # Backpropagation (using Adam optimizer)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Start session to compute Tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            epoch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size)\n",
    "            seed = seed + 1\n",
    "            \n",
    "            for i in range(num_minibatches):\n",
    "                \n",
    "                # Get next batch of training data and labels\n",
    "                minibatch_X, minibatch_Y = train.next_batch(minibatch_size)\n",
    "                \n",
    "                # Execute optimizer and cost function\n",
    "                _, minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X.T, Y: minibatch_Y.T})\n",
    "                \n",
    "                # Update epoch cost\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "                \n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True:\n",
    "                print(\"Cost after epoch {epoch_num}: {cost}\".format(epoch_num=epoch, cost=epoch_cost))\n",
    "                costs.append(epoch_cost)\n",
    "        \n",
    "        # Plot costs\n",
    "        plt.figure(figsize=(16,5))\n",
    "        plt.plot(np.squeeze(costs), color='#2A688B')\n",
    "        plt.xlim(0, num_epochs-1)\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.title(\"learning rate = {rate}\".format(rate=learning_rate))\n",
    "        plt.savefig(graph_filename, dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "        # Save parameters\n",
    "        parameters = sess.run(parameters)\n",
    "        print(\"Parameters have been trained!\")\n",
    "        \n",
    "        # Calculate correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n",
    "        \n",
    "        # Calculate accuracy on test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: train.images.T, Y: train.labels.T}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: test.images.T, Y: test.labels.T}))\n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1529198017.544296\n",
      "Cost after epoch 0: 0.301484596836397\n",
      "Cost after epoch 1: 0.1277713663895029\n",
      "Cost after epoch 2: 0.0862101811841913\n",
      "Cost after epoch 3: 0.0652558621759357\n",
      "Cost after epoch 4: 0.04952377171508533\n",
      "Cost after epoch 5: 0.03874528120525695\n",
      "Cost after epoch 6: 0.03016319935434615\n",
      "Cost after epoch 7: 0.02560288981131601\n",
      "Cost after epoch 8: 0.02050929712079357\n",
      "Cost after epoch 9: 0.016946699655180048\n",
      "Cost after epoch 10: 0.014301544346788592\n",
      "Cost after epoch 11: 0.011838002270606518\n",
      "Cost after epoch 12: 0.012870636317349717\n",
      "Cost after epoch 13: 0.009313473353248076\n",
      "Cost after epoch 14: 0.009661171658783762\n",
      "Cost after epoch 15: 0.007517265812008051\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c36350f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 0.9977818\n",
      "Test Accuracy: 0.978\n",
      "00:01:39\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "print(start)\n",
    "train = fashion_mnist.train\n",
    "test = fashion_mnist.test\n",
    "\n",
    "parameters = model(train, test, learning_rate=0.0005)\n",
    "\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000,)\n",
      "(55000,)\n",
      "(55000, 1)\n",
      "(55000, 784)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Shapes of test set\n",
    "datait = fashion_mnist.train.images \n",
    "datalt = fashion_mnist.train.labels\n",
    "datai = fashion_mnist.test.images\n",
    "datal = fashion_mnist.test.labels\n",
    "\n",
    "\n",
    "train_x = np.array(fashion_mnist.train.images)\n",
    "test_x =  np.array(fashion_mnist.test.images)\n",
    "\n",
    "train_y = np.array(fashion_mnist.train.labels)\n",
    "test_y =  np.array(fashion_mnist.test.labels)\n",
    "train_y = train_y.T\n",
    "train_y = train_y[0]\n",
    "print(train_y.shape)\n",
    "Y = train_y\n",
    "print(Y.shape)   \n",
    "train_y = (np.arange(np.max(Y)) == Y[:, None]).astype(int)\n",
    "print(train_y.shape)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fashion_nn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fashion_nn.py \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z),axis=1))\n",
    "    return sm\n",
    "\n",
    "\n",
    "def layers(X, Y):\n",
    "    \"\"\"\n",
    "    :param X:\n",
    "    :param Y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0]\n",
    "    n_y = Y.shape[0]\n",
    "    return n_x, n_y\n",
    "\n",
    "\n",
    "def initialize_nn(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    :param n_x:\n",
    "    :param n_h:\n",
    "    :param n_y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    np.random.seed(2)\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.random.rand(n_h, 1)\n",
    "    W2 = np.random.rand(n_y, n_h)\n",
    "    b2 = np.random.rand(n_y, 1)\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def forward_prop(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = softmax(Z2.T)\n",
    "\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "\n",
    "    return A2, cache\n",
    "\n",
    "\n",
    "def compute_cost(A2, Y, parameters):\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    logprobs = np.multiply(np.log(A2), Y)\n",
    "    cost = - np.sum(logprobs) / m\n",
    "    cost = np.squeeze(cost)\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def back_prop(parameters, cache, X, Y):\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "\n",
    "\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.square(A1))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "def update_params(parameters, grads, alpha):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def model_nn(label_dict,X, Y,Y_real,test_x,test_y, n_h, num_iters, alpha, print_cost):\n",
    "    np.random.seed(3)\n",
    "    n_x,n_y = layers(X, Y)\n",
    "    parameters = initialize_nn(n_x, n_h, n_y)\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    costs = []\n",
    "    for i in range(0, num_iters):\n",
    "\n",
    "        A2, cache = forward_prop(X, parameters)\n",
    "\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    "        grads = back_prop(parameters, cache, X, Y)\n",
    "        if (i > 1500):\n",
    "            alpha1 = 0.95*alpha\n",
    "            parameters = update_params(parameters, grads, alpha1)\n",
    "        else:\n",
    "            parameters = update_params(parameters, grads, alpha)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration for %i: %f\" % (i, cost))\n",
    "\n",
    "\n",
    "\n",
    "    predictions = predict_nn(parameters, X)\n",
    "    print(\"Train accuracy: {} %\", sum(predictions == Y_real) / (float(len(Y_real))) * 100)\n",
    "    predictions=predict_nn(parameters,test_x)\n",
    "    print(\"Train accuracy: {} %\", sum(predictions == test_y) / (float(len(test_y))) * 100)\n",
    "\n",
    "\n",
    "\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(alpha))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def predict_nn(parameters, X):\n",
    "    A2, cache = forward_prop(X, parameters)\n",
    "    predictions = np.argmax(A2, axis=0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fashion_LR.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fashion_LR.py \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z), axis=1))\n",
    "    return sm\n",
    "\n",
    "\n",
    "def initialize(dim1, dim2):\n",
    "    \"\"\"\n",
    "    :param dim: size of vector w initilazied with zeros\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    w = np.zeros(shape=(dim1, dim2))\n",
    "    b = np.zeros(shape=(10, 1))\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    :param w: weights for w\n",
    "    :param b: bias\n",
    "    :param X: size of data(no of features, no of examples)\n",
    "    :param Y: true label\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    m = X.shape[1]  # getting no of rows\n",
    "\n",
    "    # Forward Prop\n",
    "    A = softmax((np.dot(w.T, X) + b).T)\n",
    "    cost = (-1 / m) * np.sum(Y * np.log(A))\n",
    "\n",
    "    # backwar prop\n",
    "    dw = (1 / m) * np.dot(X, (A - Y).T)\n",
    "    db = (1 / m) * np.sum(A - Y)\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    return grads, cost\n",
    "\n",
    "\n",
    "def optimize(w, b, X, Y, num_iters, alpha, print_cost=False):\n",
    "    \"\"\"\n",
    "    :param w: weights for w\n",
    "    :param b: bias\n",
    "    :param X: size of data(no of features, no of examples)\n",
    "    :param Y: true label\n",
    "    :param num_iters: number of iterations for gradient\n",
    "    :param alpha:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    costs = []\n",
    "    for i in range(num_iters):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        w = w - alpha * dw\n",
    "        b = b - alpha * db\n",
    "        alpha = alpha * 0.99\n",
    "\n",
    "        # Record the costs\n",
    "        if i % 50 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 50 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "\n",
    "    return params, grads, costs\n",
    "\n",
    "\n",
    "def predict(w, b, X):\n",
    "    \"\"\"\n",
    "    :param w:\n",
    "    :param b:\n",
    "    :param X:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    y_pred = np.argmax(softmax((np.dot(w.T, X) + b).T), axis=0)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def model_LR(label_dict, X_train, Y_train, Y, test_x, test_y, num_iters, alpha, print_cost):\n",
    "    \"\"\"\n",
    "    :param X_train:\n",
    "    :param Y_train:\n",
    "    :param X_test:\n",
    "    :param Y_test:\n",
    "    :param num_iterations:\n",
    "    :param learning_rate:\n",
    "    :param print_cost:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    w, b = initialize(X_train.shape[0], Y_train.shape[0])\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iters, alpha, print_cost)\n",
    "\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "\n",
    "    y_prediction_train = predict(w, b, X_train)\n",
    "    y_prediction_test = predict(w, b, test_x)\n",
    "    print(\"Train accuracy: {} %\", sum(y_prediction_train == Y) / (float(len(Y))) * 100)\n",
    "    print(\"Test accuracy: {} %\", sum(y_prediction_test == test_y) / (float(len(test_y))) * 100)\n",
    "\n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": y_prediction_test,\n",
    "         \"Y_prediction_train\": y_prediction_train,\n",
    "         \"w\": w,\n",
    "         \"b\": b,\n",
    "         \"learning_rate\": alpha,\n",
    "         \"num_iterations\": num_iters}\n",
    "\n",
    "    # Plot learning curve (with costs)\n",
    "    costs = np.squeeze(d['costs'])\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "    plt.plot()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    pri(X_train, y_prediction_train, label_dict)\n",
    "    return d\n",
    "\n",
    "\n",
    "def pri(X, Y, label):\n",
    "    example = X[:, 2]\n",
    "    print(\"Prediction for the example is \", label[Y[2]])\n",
    "    plt.imshow(np.reshape(example, [28, 28]), cmap='Greys')\n",
    "    plt.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fashion_DL.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fashion_DL.py \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    cache = z\n",
    "    z -= np.max(z)\n",
    "    sm = (np.exp(z).T / np.sum(np.exp(z), axis=1))\n",
    "    return sm, cache\n",
    "\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"\n",
    "    :param z:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    s = np.maximum(0, z)\n",
    "    cache = z\n",
    "    return s, cache\n",
    "\n",
    "\n",
    "def softmax_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    :param dA:\n",
    "    :param activation_cache:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    z = cache\n",
    "    z -= np.max(z)\n",
    "    s = (np.exp(z).T / np.sum(np.exp(z), axis=1))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    :param dA:\n",
    "    :param activation_cache:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)  # just converting dz to a correct object.\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def initialize_parameters_deep(dims):\n",
    "    \"\"\"\n",
    "    :param dims:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(3)\n",
    "    params = {}\n",
    "    L = len(dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        params['W' + str(l)] = np.random.randn(dims[l], dims[l - 1]) * 0.01\n",
    "        params['b' + str(l)] = np.zeros((dims[l], 1))\n",
    "    return params\n",
    "\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    :param A:\n",
    "    :param W:\n",
    "    :param b:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    :param A_prev:\n",
    "    :param W:\n",
    "    :param b:\n",
    "    :param activation:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if activation == \"softmax\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z.T)\n",
    "\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def L_model_forward(X, params):\n",
    "    \"\"\"\n",
    "    :param X:\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(params) // 2  # number of layers in the neural network\n",
    "\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev,\n",
    "                                             params[\"W\" + str(l)],\n",
    "                                             params[\"b\" + str(l)],\n",
    "                                             activation='relu')\n",
    "        caches.append(cache)\n",
    "\n",
    "    A_last, cache = linear_activation_forward(A,\n",
    "                                              params[\"W\" + str(L)],\n",
    "                                              params[\"b\" + str(L)],\n",
    "                                              activation='softmax')\n",
    "    caches.append(cache)\n",
    "    return A_last, caches\n",
    "\n",
    "\n",
    "def compute_cost(A_last, Y):\n",
    "    \"\"\"\n",
    "    :param A_last:\n",
    "    :param Y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1 / m) * np.sum(Y * np.log(A_last))\n",
    "    cost = np.squeeze(cost)  # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    return cost\n",
    "\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    :param dZ:\n",
    "    :param cache:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1. / m) * np.dot(dZ, cache[0].T)\n",
    "    db = (1. / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(cache[1].T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    :param dA:\n",
    "    :param cache:\n",
    "    :param activation:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    elif activation == \"softmax\":\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def L_model_backward(A_last, Y, caches):\n",
    "    \"\"\"\n",
    "    :param A_last:\n",
    "    :param Y:\n",
    "    :param caches:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches)  # the number of layers\n",
    "    m = A_last.shape[1]\n",
    "    Y = Y.reshape(A_last.shape)  # after this line, Y is the same shape as A_last\n",
    "\n",
    "    dA_last = - (np.divide(Y, A_last) - np.divide(1 - Y, 1 - A_last))\n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dA_last,\n",
    "                                                                                                  current_cache,\n",
    "                                                                                                  activation=\"softmax\")\n",
    "\n",
    "    for l in reversed(range(L - 1)):\n",
    "        current_cache = caches[l]\n",
    "\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache,\n",
    "                                                                    activation=\"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "def update_params(params, grads, alpha):\n",
    "    \"\"\"\n",
    "    :param params:\n",
    "    :param grads:\n",
    "    :param alpha:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    L = len(params) // 2  # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        params[\"W\" + str(l + 1)] = params[\"W\" + str(l + 1)] - alpha * grads[\"dW\" + str(l + 1)]\n",
    "        params[\"b\" + str(l + 1)] = params[\"b\" + str(l + 1)] - alpha * grads[\"db\" + str(l + 1)]\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def model_DL(label_dict, X, Y, Y_real, test_x, test_y, layers_dims, alpha, num_iterations, print_cost):  # lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    alpha -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    Returns:\n",
    "    params -- params learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []  # keep track of cost\n",
    "\n",
    "    params = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        A_last, caches = L_model_forward(X, params)\n",
    "        cost = compute_cost(A_last, Y)\n",
    "        grads = L_model_backward(A_last, Y, caches)\n",
    "\n",
    "        if (i > 800 and i<1700):\n",
    "            alpha1 = 0.80 * alpha\n",
    "            params = update_params(params, grads, alpha1)\n",
    "        elif(i>=1700):\n",
    "            alpha1 = 0.50 * alpha\n",
    "            params = update_params(params, grads, alpha1)\n",
    "        else:\n",
    "            params = update_params(params, grads, alpha)\n",
    "\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    predictions = predict(params, X)\n",
    "    print(\"Train accuracy: {} %\", sum(predictions == Y_real) / (float(len(Y_real))) * 100)\n",
    "    predictions = predict(params, test_x)\n",
    "    print(\"Train accuracy: {} %\", sum(predictions == test_y) / (float(len(test_y))) * 100)\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(alpha))\n",
    "    plt.show()\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def predict(parameters, X):\n",
    "    A_last, cache = L_model_forward(X, parameters)\n",
    "    predictions = np.argmax(A_last, axis=0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1529198117.262235\n",
      "1529198117.262235\n",
      "1529198117.2627819\n",
      "00:00:00\n",
      "(55000,)\n",
      "(55000,)\n",
      "(55000, 1)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fashion_LR\n",
    "import fashion_nn\n",
    "import fashion_DL\n",
    "start = time.time()\n",
    "print(start)\n",
    "\n",
    "def main():\n",
    "    label_dict = {\n",
    "        0: 'T - shirt / top',\n",
    "        1: 'Trouser',\n",
    "        2: 'Pullover',\n",
    "        3: 'Dress',\n",
    "        4: 'Coat',\n",
    "        5: 'Sandal',\n",
    "        6: 'Shirt',\n",
    "        7: 'Sneaker',\n",
    "        8: 'Bag',\n",
    "        9: 'Ankleboot'\n",
    "    }\n",
    "\n",
    "    train_x = np.array(fashion_mnist.train.images)\n",
    "    test_x =  np.array(fashion_mnist.test.images)\n",
    "\n",
    "    train_y = np.array(fashion_mnist.train.labels)\n",
    "    test_y =  np.array(fashion_mnist.test.labels)\n",
    "    train_y = train_y.T\n",
    "    train_y = train_y[0]\n",
    "    print(train_y.shape)\n",
    "    Y = train_y\n",
    "    print(Y.shape)   \n",
    "    train_y = (np.arange(np.max(Y)) == Y[:, None]).astype(int)\n",
    "    print(train_y.shape)  \n",
    "\n",
    "    choice = input(\"1. Logistic Regression \\n2. Shallow Network \\n3. Deep Network\\n\")\n",
    "    if choice == '1':\n",
    "        d = fashion_LR.model_LR(label_dict, train_x.T, train_y.T, Y, test_x.T, test_y.T[0], num_iters=1500,\n",
    "                                alpha=0.000005, print_cost=True)\n",
    "    elif choice == '2':\n",
    "        d = fashion_nn.model_nn(label_dict, train_x.T, train_y.T, Y, test_x.T, test_y.T[0], n_h=100, num_iters=2300,\n",
    "                                alpha=0.005, print_cost=True)\n",
    "    elif choice == '3':\n",
    "        dims = [784, 300, 100, 50, 10]\n",
    "        d = fashion_DL.model_DL(label_dict, train_x.T, train_y.T, Y, test_x.T, test_y.T[0], dims, alpha=0.01,\n",
    "                                num_iterations=2500, print_cost=True)\n",
    "    else:\n",
    "        print(\"Invalid Choice\")\n",
    "end = time.time()\n",
    "print(start)\n",
    "print(end)\n",
    "elapsed_time = end - start\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "main()\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(elapsed)\n",
    "elapsed_time = end - start\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from nolearn.dbn import DBN\n",
    "import timeit\n",
    "train = fashion_mnist\n",
    "features = train.columns[1:]\n",
    "X = train[features]\n",
    "y = train['label']\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X/255.,y,test_size=0.1,random_state=0)\n",
    "\n",
    "train_x = np.array(fashion_mnist.train.images)\n",
    "test_x =  np.array(fashion_mnist.test.images)\n",
    "\n",
    "train_y = np.array(fashion_mnist.train.labels)\n",
    "test_y =  np.array(fashion_mnist.test.labels)\n",
    "train_y = train_y.T\n",
    "train_y = train_y[0]\n",
    "print(train_y.shape)\n",
    "Y = train_y\n",
    "print(Y.shape)   \n",
    "train_y = (np.arange(np.max(Y)) == Y[:, None]).astype(int)\n",
    "print(train_y.shape)  \n",
    "    \n",
    "clf_rf = RandomForestClassifier()\n",
    "clf_rf.fit(train_x,train_y)\n",
    "y_pred_rf = clf_rf.predict(test_y)\n",
    "acc_rf = accuracy_score(test_y, y_pred_rf)\n",
    "print(\"random forest accuracy: \",acc_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = end - start\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mnist_reader.py \n",
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import libraries\n",
    "from mnist_reader import load_mnist\n",
    "from keras.layers import Dense, MaxPool2D, Conv2D, Dropout\n",
    "from keras.layers import Flatten, InputLayer\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.initializers import Constant\n",
    "\n",
    "\n",
    "\n",
    "# Load data\n",
    "# Function load_minst is available in git.\n",
    "X_train = np.array(fashion_mnist.train.images)\n",
    "X_test =  np.array(fashion_mnist.test.images)\n",
    "\n",
    "y_train = np.array(fashion_mnist.train.labels)\n",
    "y_test =  np.array(fashion_mnist.test.labels)\n",
    "\n",
    "\n",
    "# Prepare datasets\n",
    "# This step contains normalization and reshaping of input.\n",
    "# For output, it is important to change number to one-hot vector. \n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.astype('float32') / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "# Create model in Keras\n",
    "# This model is linear stack of layers\n",
    "clf = Sequential()\n",
    "# This layer is used as an entry point into a graph. \n",
    "# So, it is important to define input_shape.\n",
    "clf.add(\n",
    "    InputLayer(input_shape=(1, 28, 28))\n",
    ")\n",
    "# Normalize the activations of the previous layer at each batch.\n",
    "clf.add(\n",
    "    BatchNormalization()\n",
    ")\n",
    "# Next step is to add convolution layer to model.\n",
    "clf.add(\n",
    "    Conv2D(\n",
    "        32, (2, 2), \n",
    "        padding='same', \n",
    "        bias_initializer=Constant(0.01), \n",
    "        kernel_initializer='random_uniform'\n",
    "    )\n",
    ")\n",
    "# Add max pooling layer for 2D data.\n",
    "clf.add(MaxPool2D(padding='same'))\n",
    "# Add this same two layers to model.\n",
    "clf.add(\n",
    "    Conv2D(\n",
    "        32, \n",
    "        (2, 2), \n",
    "        padding='same', \n",
    "        bias_initializer=Constant(0.01), \n",
    "        kernel_initializer='random_uniform', \n",
    "        input_shape=(1, 28, 28)\n",
    "    )\n",
    ")\n",
    "clf.add(MaxPool2D(padding='same'))\n",
    "# It is necessary to flatten input data to a vector.\n",
    "clf.add(Flatten())\n",
    "# Last step is creation of fully-connected layers.\n",
    "clf.add(\n",
    "    Dense(\n",
    "        128,\n",
    "        activation='relu',\n",
    "        bias_initializer=Constant(0.01), \n",
    "        kernel_initializer='random_uniform',         \n",
    "    )\n",
    ")\n",
    "# Add output layer, which contains ten numbers.\n",
    "# Each number represents cloth type.\n",
    "clf.add(Dense(10, activation='softmax'))\n",
    "# Last step in Keras is to compile model.\n",
    "clf.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(clf.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "clf.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "clf.evaluate(X_test, y_test)\n",
    "\n",
    "clf.metrics_names\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
