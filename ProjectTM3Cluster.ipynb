{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, argparse\n",
    "import pandas as p\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('/Users/kenmckee/Desktop/GS/S18/tm/Project/PJ/CKOne.json') as fd:\n",
    "    script = json.load(fd)\n",
    "    \n",
    "    #print(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_TYPES=['character', 'speech', 'stage direction', 'location']\n",
    "CHARACTER=0\n",
    "SPEECH=1\n",
    "DIRECTIONS=2\n",
    "LOCATION=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_characters(script):\n",
    "    '''\n",
    "    Extracts the (unique) characters list from the script\n",
    "    '''\n",
    "    characters=[]\n",
    "    for block in script['movie_script']:\n",
    "        if(block['type'] == BLOCK_TYPES[SPEECH]):\n",
    "            character = block['character']\n",
    "            if not character in characters:\n",
    "                characters.append(character)\n",
    "\n",
    "    return characters\n",
    "\n",
    "def extract_locations(script):\n",
    "    '''\n",
    "    Extracts the (unique) locations list from the script\n",
    "    '''\n",
    "    locations=[]\n",
    "    for block in script['movie_script']:\n",
    "        if(block['type'] == BLOCK_TYPES[LOCATION]):\n",
    "            location = block['text']\n",
    "            if not location in locations:\n",
    "                locations.append(location)\n",
    "\n",
    "    return locations\n",
    "\n",
    "def extract_directions(script):\n",
    "    '''\n",
    "    Extracts the stage directions list from the script\n",
    "    '''\n",
    "    directions=[]\n",
    "    for block in script['movie_script']:\n",
    "        if(block['type'] == BLOCK_TYPES[DIRECTIONS]):\n",
    "            directions.append(block['text'])\n",
    "\n",
    "    return directions\n",
    "\n",
    "def extract_speech_given_character(script, character, strict_match=False):\n",
    "    '''\n",
    "    Extracts the given character's utterances from the script\n",
    "\n",
    "    If strict_match is True, we will only extract utterances that perfectly match (==) the parameter;\n",
    "    otherwise, we will extract utterances whose character partly matches (in) the parameter.\n",
    "    In both cases, the match is case-insensitive.\n",
    "\n",
    "    Also asks the user wether one wants to keep the character's name before each utterance.\n",
    "    '''\n",
    "\n",
    "    speeches=[]\n",
    "    for block in script['movie_script']:\n",
    "        if( block['type'] == BLOCK_TYPES[SPEECH] and\n",
    "            (strict_match and (character.lower() == block['character'].lower()) or\n",
    "             not strict_match and (character.lower() in block['character'].lower())) ):\n",
    "            if( keep_character_name ):\n",
    "                speeches.append(block['character'])\n",
    "            speeches.append(block['text'])\n",
    "\n",
    "    return speeches\n",
    "\n",
    "def extract_all_characters_speech(script):\n",
    "    '''\n",
    "    Extracts all speeches from the script\n",
    "    '''\n",
    "    return extract_speech_given_character(script, '')\n",
    "\n",
    "def extract_speech_asking_user(script):\n",
    "    '''\n",
    "    Extracts utterances by asking the user which character one wants to get\n",
    "\n",
    "    Also asks wether the user wants a perfect (==) or partial (in) match.\n",
    "    '''\n",
    "    character = input_string('Please provide the name of the character: ')\n",
    "\n",
    "    strict_match = False\n",
    "    answer = input('Do you want utterances of this exact character (or any character that matches \\'{}\\')? (y/N) '.format(character))\n",
    "    if( answer == 'y' or answer =='Y' ):\n",
    "        strict_match = True\n",
    "\n",
    "    return extract_speech_given_character(script, character, strict_match)\n",
    "\n",
    "def extract_speech_using_characters_list(script):\n",
    "    '''\n",
    "    Extracts utterances by providing the user with the characters list\n",
    "\n",
    "    Also asks wether the user wants a perfect (==) or partial (in) match.\n",
    "    '''\n",
    "    characters = sorted(extract_characters(script))\n",
    "    character = characters[input_from_list('Please choose a character:', characters)]\n",
    "\n",
    "    strict_match = False\n",
    "    answer = input('Do you want utterances of this exact character (or any character that matches \\'{}\\')? (y/N) '.format(character))\n",
    "    if( answer == 'y' or answer =='Y' ):\n",
    "        strict_match = True\n",
    "\n",
    "    return extract_speech_given_character(script, character, strict_match)\n",
    "\n",
    "def extract_speech(script):\n",
    "    '''\n",
    "    Asks the user which speeches one wants to extract, and calls the appropriate function\n",
    "    '''\n",
    "    speech=[]\n",
    "\n",
    "    choices = ['all characters',\n",
    "               'give the character\\'s name',\n",
    "               'choose from the characters list']\n",
    "\n",
    "    action = input_from_list(\"Which character speeches do you want to extract?\", choices)\n",
    "\n",
    "    if( action == 0 ):\n",
    "        return extract_all_characters_speech(script)\n",
    "    elif( action == 1 ):\n",
    "        return extract_speech_asking_user(script)\n",
    "    elif( action == 2 ):\n",
    "        return extract_speech_using_characters_list(script)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"KANE'S OLD OLD VOICE\", 'NEWS DIGEST NARRATOR', '(DROPPING THE QUOTES)', 'NARRATOR', 'THATCHER', 'INVESTIGATOR', 'SPEAKER', 'THIRD MAN', \"THOMPSON'S VOICE\", \"A MAN'S VOICE\", 'A VOICE', 'THE VOICE', 'ANOTHER VOICE', \"RAWLSTON'S VOICE\", 'RAWLSTON', \"SOMEBODY'S VOICE\", 'THOMPSON', 'THE CAPTAIN', 'SUSAN', 'THE WAITER', 'BERTHA', 'GUARD', 'MRS. KANE', \"THATCHER'S VOICE\", 'KANE SR.', 'KANE', 'BERNSTEIN', 'MIKE', 'MISS ANDERSON', 'CITY EDITOR', 'HIRELING', 'ANOTHER HIRELING', \"KANE'S VOICE\", 'CARTER', 'LELAND', 'SMATHERS', \"NEWSBOYS' VOICES\", 'PHOTOGRAPHER', 'THE OFFICIAL', \"STEWARD'S VOICE\", 'MISS TOWNSEND', '(A PAUSE)', \"LELAND'S VOICE\", 'GEORGIE', 'ETHEL', 'ONE OF THE GIRLS', 'ANOTHER GIRL', 'THE PRESIDENT', 'FOREMAN', 'REILLY', 'THE COP', \"MIKE'S VOICE\", 'A VOICE FROM THE CROWD', 'EMILY', 'FIRST CIVIC LEADER', 'SECOND LEADER', 'JUNIOR', 'THE DRIVER', 'THE MAID', 'ROGERS', '(ANOTHER PAUSE)', 'NURSE', 'MATISTI', \"RAYMOND'S VOICE\", 'RAYMOND', 'DR. COREY', \"BERNSTEIN'S VOICE\", \"CHARLIE JR.'S VOICE\", 'MARIE', 'FRED', 'KATHERINE', 'ONE OF THE WORKMEN', 'ASSISTANT', 'THIRD NEWSPAPERMAN', 'SECOND ASSISTANT', 'GIRL', 'SECOND NEWSPAPERMAN']\n"
     ]
    }
   ],
   "source": [
    "result_character = extract_characters(script)\n",
    "result_direction = extract_directions(script)\n",
    "result_location = extract_locations(script)\n",
    "\n",
    "print(result_character)\n",
    "\n",
    "#print(result_location)\n",
    "#print(result_location)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_character_name = True\n",
    "strict_match = False\n",
    "\n",
    "text=[]\n",
    "characters=[]\n",
    "for character in result_character:\n",
    "    for block in script['movie_script']:\n",
    "        if( block['type'] == BLOCK_TYPES[SPEECH] and\n",
    "            (strict_match and (character.lower() == block['character'].lower()) or\n",
    "             not strict_match and (character.lower() in block['character'].lower())) ):\n",
    "            if( keep_character_name ):\n",
    "                characters.append(block['character'])\n",
    "                text.append(block['text'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivoted to numbers and combined some pesky records together. \n",
    "cleaned up some unmanageable formats and built an input csv for posterity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pre=p.read_csv(\"/Users/kenmckee/Desktop/pri.csv\", delimiter=',') \n",
    "\n",
    "#print(pre)\n",
    "pre = pre.replace('\\n',' ', regex=True)\n",
    "pre = pre.replace('\\r',' ', regex=True)\n",
    "pre = pre.replace('\\.','', regex=True)\n",
    "\n",
    "ccorpus = pre.groupby('char')['text'].apply(' '.join).reset_index()\n",
    "#ccorpus = ccorpus.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "char=ccorpus['char'].values \n",
    "text=ccorpus['text'].values\n",
    "#y=pre['char'].values \n",
    "#X=pre['text'].values\n",
    "\n",
    "#print(ccorpus.groups)\n",
    "\n",
    "#print(pre)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = 'say','what','are','you','mr','did','say'\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(['say','what','are','you','mr','did','say'])\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if token not in stopwords:\n",
    "                filtered_tokens.append(token)\n",
    "                stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if token not in stopwords:\n",
    "            if re.search('[a-zA-Z]', token):\n",
    "                filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "def stopword(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_stop = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords:\n",
    "             filtered_tokens.append(token)\n",
    "    return text_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is to analyze the characters\n",
    "#not super pythonic, no, not at all.\n",
    "#use extend so it's a big flat list of vocab\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "for i in text:\n",
    "    \n",
    "    \n",
    "   # allwords_stemmed = tokenize_and_stem(i) #for each item in 'courseDesc', tokenize/stem\n",
    "   # totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 158 ms, sys: 3.7 ms, total: 162 ms\n",
      "Wall time: 161 ms\n",
      "(61, 208)\n",
      "                             terms\n",
      "'d                              'd\n",
      "'ll                            'll\n",
      "'m                              'm\n",
      "'m going                  'm going\n",
      "'re                            're\n",
      "'re going                're going\n",
      "'s                              's\n",
      "'s 's                        's 's\n",
      "'s going                  's going\n",
      "'s got                      's got\n",
      "'s right                  's right\n",
      "'ve                            've\n",
      "'ve got                    've got\n",
      "ago                            ago\n",
      "alexander                alexander\n",
      "american                  american\n",
      "answer                      answer\n",
      "anybody                    anybody\n",
      "ask                            ask\n",
      "away                          away\n",
      "believe                    believe\n",
      "bernstein                bernstein\n",
      "best                          best\n",
      "better                      better\n",
      "big                            big\n",
      "built                        built\n",
      "business                  business\n",
      "ca                              ca\n",
      "ca n't                      ca n't\n",
      "called                      called\n",
      "...                            ...\n",
      "thought                    thought\n",
      "thousand                  thousand\n",
      "thousand dollars  thousand dollars\n",
      "time                          time\n",
      "today                        today\n",
      "told                          told\n",
      "tonight                    tonight\n",
      "tough                        tough\n",
      "turns                        turns\n",
      "understand              understand\n",
      "used                          used\n",
      "voice                        voice\n",
      "want                          want\n",
      "wanted                      wanted\n",
      "way                            way\n",
      "week                          week\n",
      "window                      window\n",
      "wish                          wish\n",
      "wo                              wo\n",
      "wo n't                      wo n't\n",
      "word                          word\n",
      "words                        words\n",
      "work                          work\n",
      "world                        world\n",
      "xanadu                      xanadu\n",
      "years                        years\n",
      "yes                            yes\n",
      "yesterday                yesterday\n",
      "york                          york\n",
      "young                        young\n",
      "\n",
      "[208 rows x 1 columns]\n",
      "[\"'d\", \"'ll\", \"'m\", \"'m going\", \"'re\", \"'re going\", \"'s\", \"'s 's\", \"'s going\", \"'s got\", \"'s right\", \"'ve\", \"'ve got\", 'ago', 'alexander', 'american', 'answer', 'anybody', 'ask', 'away', 'believe', 'bernstein', 'best', 'better', 'big', 'built', 'business', 'ca', \"ca n't\", 'called', 'calling', 'came', 'chance', 'charles', 'charles foster', 'charles foster kane', 'charlie', 'chicago', 'chronicle', 'city', 'colorado', 'come', 'comes', 'continue', 'course', 'day', 'days', 'dead', 'death', 'dollars', 'door', 'emily', 'end', 'enquirer', 'exactly', 'fact', 'forget', 'foster', 'foster kane', 'girl', 'glad', 'going', 'good', 'got', 'governor', 'great', 'guess', 'happen', 'heard', 'hello', 'home', 'hours', 'house', 'idea', 'interested', 'kane', \"kane 's\", \"kane n't\", 'kind', 'knew', 'know', 'leave', 'leaving', 'left', 'leland', 'let', \"let 's\", 'life', 'like', 'listen', 'little', 'live', 'long', 'look', 'looking', 'looks', 'lost', 'lot', 'love', 'loved', 'make', 'man', \"man 's\", 'matter', 'matter fact', 'maybe', 'mean', 'men', 'mind', 'minute', 'miss', 'money', 'morning', 'mother', 'mrs', 'mrs kane', \"n't\", \"n't know\", \"n't make\", \"n't think\", 'new', 'new york', 'news', 'newspaper', 'nice', 'night', 'note', 'oh', 'oh yes', 'old', 'opera', 'ought', 'paper', 'papers', 'pause', 'pauses', 'people', 'place', 'president', \"president 's\", 'pretty', 'private', 'probably', 'quietly', 'read', 'reading', 'really', 'remember', 'right', \"right 's\", 'right away', 'rises', 'rosebud', 'run', 'said', 'send', 'sent', 'sit', 'slowly', 'somebody', 'son', 'sorry', 'stand', 'starts', 'state', 'stop', 'stops', 'story', 'street', 'sure', 'susan', 'talk', 'tell', 'thatcher', 'thing', 'things', 'think', 'thompson', 'thought', 'thousand', 'thousand dollars', 'time', 'today', 'told', 'tonight', 'tough', 'turns', 'understand', 'used', 'voice', 'want', 'wanted', 'way', 'week', 'window', 'wish', 'wo', \"wo n't\", 'word', 'words', 'work', 'world', 'xanadu', 'years', 'yes', 'yesterday', 'york', 'young']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=50, max_features=1000,\n",
    "                                 min_df=5, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_only, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(text) #fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "terms = p.DataFrame({'terms': tfidf_vectorizer.get_feature_names()}, index = tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "terms.reindex\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "print\n",
    "print(terms)\n",
    "print(tfidf_vectorizer.get_feature_names())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 9228 items in vocab_frame\n",
      "             words\n",
      "'s              's\n",
      "tough        tough\n",
      "thing        thing\n",
      "newsreel  newsreel\n",
      "seventy    seventy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_frame = p.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_tokenized)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')\n",
    "\n",
    "vocab_frame.reindex\n",
    "print(vocab_frame.head())\n",
    "print\n",
    "print\n",
    "print\n",
    "print\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 195 ms, sys: 3.19 ms, total: 199 ms\n",
      "Wall time: 199 ms\n",
      "[1, 0, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 3\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#uncomment the below to save your model \n",
    "#since I've already run my model I am loading from the pickle\n",
    "\n",
    "#joblib.dump(km,  'doc_cluster.pkl')\n",
    "\n",
    "#km = joblib.load('doc_cluster.pkl')\n",
    "#clusters = km.labels_.tolist()\n",
    "\n",
    "#joblib.dump(tkm,  'doc_cluster.pkl')\n",
    "\n",
    "#km = joblib.load('doc_cluster.pkl')\n",
    "#clusters = tkm.labels_.tolist()\n",
    "\n",
    "\n",
    "#print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "films = { 'char': char,'text': text, 'cluster': clusters}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os  # for os.path.basename\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "MDS()\n",
    "\n",
    "# convert two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "print()\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "36",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3063\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 36",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-127c07304900>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morder_centroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#replace 6 with n words per cluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvocab_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#add whitespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#add whitespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2690\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2692\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3063\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3065\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 36"
     ]
    }
   ],
   "source": [
    "films = { 'title': char,'text': text, 'cluster': clusters}\n",
    "frame = p.DataFrame(films, index = [clusters] , columns = ['title', 'text'])\n",
    "framex = p.DataFrame(films,  columns = ['title', 'text','cluster'])\n",
    "\n",
    "\n",
    "#frame['cluster'].value_counts() #number of films per cluster (clusters from 0 to 4)\n",
    "\n",
    "#print(frame)\n",
    "\n",
    "\n",
    "\n",
    "grouped = frame.groupby(frame['title']) #groupby cluster for aggregation purposes\n",
    "\n",
    "#grouped.mean() #average rank (1 to 100) per cluster\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :10]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in frame.ix[i]['title'].values.tolist():\n",
    "        print(' %s,' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in order_centroids[i, :10]: #replace 6 with n words per cluster\n",
    "        print(' %s' % terms[ind].values.tolist()[0][0], end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in framex['char'].values.tolist():\n",
    "        print(' %s,' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = p.DataFrame(films, index = [clusters] , columns = ['char', 'text'])\n",
    "framex = p.DataFrame(films,  columns = ['char', 'text','cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framex['cluster'].value_counts() #number of films per cluster (clusters from 0 to 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up colors per clusters using a dict\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: 'kane, did, yes', \n",
    "                 1: 'mr, kane, did', \n",
    "                 2: 'kane, know, did',\n",
    "                 3: 'mr, kane, know',\n",
    "                 3: 'mr, kane, know'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some ipython magic to show the matplotlib plots inline\n",
    "%matplotlib inline \n",
    "\n",
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = p.DataFrame(dict(x=xs, y=ys, label=clusters, title=char)) \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
    "            label=cluster_names[name], color=cluster_colors[name], \n",
    "            mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelbottom='off')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelleft='off')\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=8)  \n",
    "\n",
    "    \n",
    "    \n",
    "plt.show() #show the plot\n",
    "\n",
    "#uncomment the below to save the plot if need be\n",
    "#plt.savefig('clusters_small_noaxes.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n test vectors\")\n",
    "training_labels = set(char)\n",
    "print(training_labels)\n",
    "from scipy.stats import itemfreq\n",
    "training_category_dist = np.unique(char, return_counts=True)\n",
    "\n",
    "print(training_category_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "#define custom toolbar location\n",
    "class TopToolbar(mpld3.plugins.PluginBase):\n",
    "    \"\"\"Plugin for moving toolbar to top of figure\"\"\"\n",
    "\n",
    "    JAVASCRIPT = \"\"\"\n",
    "    mpld3.register_plugin(\"toptoolbar\", TopToolbar);\n",
    "    TopToolbar.prototype = Object.create(mpld3.Plugin.prototype);\n",
    "    TopToolbar.prototype.constructor = TopToolbar;\n",
    "    function TopToolbar(fig, props){\n",
    "        mpld3.Plugin.call(this, fig, props);\n",
    "    };\n",
    "\n",
    "    TopToolbar.prototype.draw = function(){\n",
    "      // the toolbar svg doesn't exist\n",
    "      // yet, so first draw it\n",
    "      this.fig.toolbar.draw();\n",
    "\n",
    "      // then change the y position to be\n",
    "      // at the top of the figure\n",
    "      this.fig.toolbar.toolbar.attr(\"x\", 150);\n",
    "      this.fig.toolbar.toolbar.attr(\"y\", 400);\n",
    "\n",
    "      // then remove the draw function,\n",
    "      // so that it is not called again\n",
    "      this.fig.toolbar.draw = function() {}\n",
    "    }\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.dict_ = {\"type\": \"toptoolbar\"}\n",
    "\n",
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = p.DataFrame(dict(x=xs, y=ys, label=clusters, title=char)) \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "#define custom css to format the font and to remove the axis labeling\n",
    "css = \"\"\"\n",
    "text.mpld3-text, div.mpld3-tooltip {\n",
    "  font-family:Arial, Helvetica, sans-serif;\n",
    "}\n",
    "\n",
    "g.mpld3-xaxis, g.mpld3-yaxis {\n",
    "display: none; }\n",
    "\n",
    "svg.mpld3-figure {\n",
    "margin-left: -200px;}\n",
    "\"\"\"\n",
    "\n",
    "# Plot \n",
    "fig, ax = plt.subplots(figsize=(18,10)) #set plot size\n",
    "ax.margins(0.03) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    points = ax.plot(group.x, group.y, marker='o', linestyle='', ms=18, \n",
    "                     label=cluster_names[name], mec='none', \n",
    "                     color=cluster_colors[name])\n",
    "    ax.set_aspect('auto')\n",
    "    labels = [i for i in group.title]\n",
    "    \n",
    "    #set tooltip using points, labels and the already defined 'css'\n",
    "    tooltip = mpld3.plugins.PointHTMLTooltip(points[0], labels,\n",
    "                                       voffset=10, hoffset=10, css=css)\n",
    "    #connect tooltip to fig\n",
    "    mpld3.plugins.connect(fig, tooltip, TopToolbar())    \n",
    "    \n",
    "    #set tick marks as blank\n",
    "    ax.axes.get_xaxis().set_ticks([])\n",
    "    ax.axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    #set axis as blank\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    \n",
    "ax.legend(numpoints=1) #show legend with only one dot\n",
    "\n",
    "mpld3.display() #show the plot\n",
    "\n",
    "#uncomment the below to export to html\n",
    "#html = mpld3.fig_to_html(fig)\n",
    "#print(html)\n",
    "\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=char);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "\n",
    "#uncomment below to save figure\n",
    "plt.savefig('ward_clusters.png', dpi=200) #save figure as ward_clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(framex.loc[framex['cluster'] == 0])\n",
    "print(framex.loc[framex['cluster'] == 1])\n",
    "print(framex.loc[framex['cluster'] == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fd = open(out_filename, 'w')\n",
    "#fd.write(result_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities \n",
    "\n",
    "\n",
    "#tokenize\n",
    "%time tokenized_text = [tokenize_and_stem(text) for text in pre]\n",
    "\n",
    "#remove stop words\n",
    "%time texts = [[word for word in text if word not in stopwords] for text in tokenized_text]\n",
    "\n",
    "#create a Gensim dictionary from the texts\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "#remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "\n",
    "#convert the dictionary to a bag of words corpus for reference\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "%time \n",
    "lda = models.LdaModel(corpus, num_topics=8, \n",
    "                            id2word=dictionary, \n",
    "                            update_every=5, \n",
    "                            chunksize=10000, \n",
    "                            passes=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics_matrix = lda.show_topics(formatted=False, num_words=5)\n",
    "topics_matrix = np.array(topics_matrix)\n",
    "\n",
    "topic_words = topics_matrix[:,:,1]\n",
    "for i in topic_words:\n",
    "    print([str(word) for word in i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
